{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalute whether there appears to be Neural Collapse in policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.experimental.wrappers import RecordVideoV0\n",
    "from orbax.checkpoint import PyTreeCheckpointer, CheckpointManager\n",
    "from pprint import pprint\n",
    "\n",
    "import _pickle as pickle\n",
    "import jax\n",
    "import json\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import timeit\n",
    "\n",
    "from jaxl.buffers import get_buffer\n",
    "from jaxl.constants import *\n",
    "from jaxl.models import (\n",
    "    get_model,\n",
    "    get_policy,\n",
    "    policy_output_dim,\n",
    ")\n",
    "from jaxl.models.policies import MultitaskPolicy\n",
    "from jaxl.envs import get_environment\n",
    "from jaxl.envs.rollouts import EvaluationRollout\n",
    "from jaxl.utils import (\n",
    "    set_seed,\n",
    "    parse_dict,\n",
    "    set_dict_value,\n",
    "    get_dict_value,\n",
    "    RunningMeanStd,\n",
    "    get_device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_seed = 0\n",
    "device = \"gpu:1\"\n",
    "get_device(device)\n",
    "set_seed(run_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env(agent_path):\n",
    "    agent_config_path = os.path.join(agent_path, \"config.json\")\n",
    "    with open(agent_config_path, \"r\") as f:\n",
    "        agent_config_dict = json.load(f)\n",
    "    return agent_config_dict[\"learner_config\"][\"env_config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/bryanpu1/projects/rl_nc_representation/jaxl\"\n",
    "log_path = os.path.join(base_path, \"jaxl/logs\")\n",
    "project_name = \"cartpole\"\n",
    "run_name = (\n",
    "    \"ppo-03-05-24_17_28_52-0d5db8df-df17-47e1-9802-8ddeec98c0b2\"\n",
    ")\n",
    "\n",
    "agent_path = (\n",
    "    agent_to_load_env_path\n",
    ") = os.path.join(\n",
    "    log_path,\n",
    "    project_name,\n",
    "    run_name,\n",
    ")\n",
    "trained_env_parameters = get_env(agent_to_load_env_path)\n",
    "\n",
    "num_episodes = 10\n",
    "env_seed = 9999\n",
    "buffer_size = 0\n",
    "num_seeds = 50\n",
    "record_video = False\n",
    "exp_name = \"-\".join(run_name.split(\"-\")[:-8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(agent_path):\n",
    "    agent_config_path = os.path.join(agent_path, \"config.json\")\n",
    "    with open(agent_config_path, \"r\") as f:\n",
    "        agent_config_dict = json.load(f)\n",
    "        agent_config_dict[\"learner_config\"][\"env_config\"] = trained_env_parameters\n",
    "        agent_config_dict[\"learner_config\"][\"env_config\"][\"env_kwargs\"][\n",
    "            \"render_mode\"\n",
    "        ] = \"rgb_array\"\n",
    "        if \"policy_distribution\" not in agent_config_dict[\"learner_config\"]:\n",
    "            agent_config_dict[\"learner_config\"][\n",
    "                \"policy_distribution\"\n",
    "            ] = CONST_DETERMINISTIC\n",
    "        set_dict_value(agent_config_dict, \"vmap_all\", False)\n",
    "        (multitask, num_models) = get_dict_value(agent_config_dict, \"num_models\")\n",
    "        agent_config = parse_dict(agent_config_dict)\n",
    "    return agent_config, {\n",
    "        \"multitask\": multitask,\n",
    "        \"num_models\": num_models,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxl.buffers.ram_buffers import NextStateNumPyBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/api_refactor.html to migrate by May 1st, 2024.\n",
      "/home/bryanpu1/.conda/envs/rl_nc/lib/python3.9/site-packages/orbax/checkpoint/type_handlers.py:1475: UserWarning: Couldn't find sharding info under RestoreArgs. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file instead of directly from RestoreArgs.\n",
      "  warnings.warn(\n",
      "/home/bryanpu1/.conda/envs/rl_nc/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.control_mode to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.control_mode` for environment variables or `env.get_wrapper_attr('control_mode')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n",
      "100%|██████████| 10/10 [00:08<00:00,  1.17it/s]\n"
     ]
    }
   ],
   "source": [
    "checkpoint_manager = CheckpointManager(\n",
    "    os.path.join(agent_path, \"models\"),\n",
    "    PyTreeCheckpointer(),\n",
    ")\n",
    "params = checkpoint_manager.restore(checkpoint_manager.latest_step())\n",
    "model_dict = params[CONST_MODEL_DICT]\n",
    "agent_policy_params = model_dict[CONST_MODEL][CONST_POLICY]\n",
    "agent_obs_rms = False\n",
    "if CONST_OBS_RMS in params:\n",
    "    agent_obs_rms = RunningMeanStd()\n",
    "    agent_obs_rms.set_state(params[CONST_OBS_RMS])\n",
    "\n",
    "agent_config, aux = get_config(agent_path)\n",
    "env = get_environment(agent_config.learner_config.env_config)\n",
    "\n",
    "buffer = NextStateNumPyBuffer(\n",
    "    buffer_size=100000,\n",
    "    obs_dim=env.observation_space.shape,\n",
    "    act_dim=env.act_dim,\n",
    "    rew_dim=env.reward_dim,\n",
    "    h_state_dim=(1,),\n",
    "    rng=np.random.RandomState(42)\n",
    ")\n",
    "\n",
    "if record_video:\n",
    "    env = RecordVideoV0(env, f\"videos/{exp_name}-videos\")\n",
    "\n",
    "input_dim = env.observation_space.shape\n",
    "output_dim = policy_output_dim(env.act_dim, agent_config.learner_config)\n",
    "model = get_model(\n",
    "    input_dim,\n",
    "    output_dim,\n",
    "    getattr(agent_config.model_config, \"policy\", agent_config.model_config),\n",
    ")\n",
    "policy = get_policy(model, agent_config.learner_config)\n",
    "if aux[\"multitask\"]:\n",
    "    policy = MultitaskPolicy(policy, model, aux[\"num_models\"])\n",
    "\n",
    "agent_rollout = EvaluationRollout(env, seed=env_seed)\n",
    "agent_rollout.rollout(\n",
    "    agent_policy_params, policy, agent_obs_rms, num_episodes, buffer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_acts = buffer.actions[:buffer.pointer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_acts, _ = np.unique(all_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "namespace(logging_config=namespace(save_path='./logs/cartpole',\n",
       "                                   experiment_name='ppo',\n",
       "                                   log_interval=10,\n",
       "                                   checkpoint_interval=100),\n",
       "          model_config=namespace(policy=namespace(architecture='mlp',\n",
       "                                                  layers=[64, 64],\n",
       "                                                  activation='tanh'),\n",
       "                                 vf=namespace(architecture='mlp',\n",
       "                                              layers=[64, 64],\n",
       "                                              activation='tanh')),\n",
       "          optimizer_config=namespace(policy=namespace(optimizer='adam',\n",
       "                                                      lr=namespace(scheduler='constant_schedule',\n",
       "                                                                   scheduler_kwargs=namespace(value=0.0003)),\n",
       "                                                      max_grad_norm=False),\n",
       "                                     vf=namespace(optimizer='adam',\n",
       "                                                  lr=namespace(scheduler='constant_schedule',\n",
       "                                                               scheduler_kwargs=namespace(value=0.0003)),\n",
       "                                                  max_grad_norm=False)),\n",
       "          learner_config=namespace(task='reinforcement_learning',\n",
       "                                   env_config=namespace(env_type='gym',\n",
       "                                                        env_name='CartPole-v1',\n",
       "                                                        env_kwargs=namespace(render_mode='rgb_array')),\n",
       "                                   seeds=namespace(model_seed=42,\n",
       "                                                   buffer_seed=42,\n",
       "                                                   env_seed=42),\n",
       "                                   buffer_config=namespace(buffer_type='default',\n",
       "                                                           buffer_size=256),\n",
       "                                   num_steps_per_epoch=2048,\n",
       "                                   learner='ppo',\n",
       "                                   gamma=0.99,\n",
       "                                   gae_lambda=0.95,\n",
       "                                   normalize_advantage=True,\n",
       "                                   eps=1e-05,\n",
       "                                   obs_rms=True,\n",
       "                                   value_rms=False,\n",
       "                                   policy_distribution='softmax',\n",
       "                                   temperature=1.0,\n",
       "                                   min_std=1e-06,\n",
       "                                   opt_batch_size=64,\n",
       "                                   opt_epochs=200,\n",
       "                                   kl_threshold=False,\n",
       "                                   update_before_early_stopping=False,\n",
       "                                   ent_loss_setting=namespace(scheduler='linear_schedule',\n",
       "                                                              scheduler_kwargs=namespace(init_value=0.002,\n",
       "                                                                                         end_value=0.0,\n",
       "                                                                                         transition_begin=0,\n",
       "                                                                                         transition_steps=100)),\n",
       "                                   vf_loss_setting=namespace(coefficient=0.5,\n",
       "                                                             reduction='mean',\n",
       "                                                             clip_param=False),\n",
       "                                   pi_loss_setting=namespace(objective='reverse_kl',\n",
       "                                                             coefficient=1.0,\n",
       "                                                             reduction='mean',\n",
       "                                                             clip_param=0.2,\n",
       "                                                             beta=0.002)),\n",
       "          train_config=namespace(num_epochs=1000))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['policy', 'vf'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[CONST_MODEL_DICT][CONST_MODEL].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxl.models.common import get_activation\n",
    "from jaxl.models.modules import MLPModule\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "def get_latent(params, inputs, carries):\n",
    "    _, mlp_states = MLPModule(\n",
    "        agent_config.model_config.policy.layers,\n",
    "        get_activation(CONST_RELU),\n",
    "        get_activation(CONST_IDENTITY),\n",
    "        use_batch_norm=False,\n",
    "    ).apply(\n",
    "        {\"params\": params[CONST_MODEL_DICT][CONST_MODEL][CONST_POLICY][CONST_PARAMS]},\n",
    "        inputs,\n",
    "        eval=True,\n",
    "        capture_intermediates=True,\n",
    "        mutable=[\"mlp_latents\"],\n",
    "    )\n",
    "\n",
    "    latents = OrderedDict()\n",
    "    for states, key in [\n",
    "        (mlp_states, \"mlp_latents\"),\n",
    "    ]:\n",
    "        for state, state_val in states[key].items():\n",
    "            latents[state] = state_val\n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = jax.vmap(get_latent, in_axes=[None, 0, 0])(\n",
    "    params,\n",
    "    buffer.observations[:buffer.pointer],\n",
    "    buffer.hidden_states[:buffer.pointer],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
