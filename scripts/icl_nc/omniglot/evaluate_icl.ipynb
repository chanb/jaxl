{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ICL Model on the Omniglot Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxl.constants import *\n",
    "from jaxl.datasets import get_dataset\n",
    "from jaxl.datasets.wrappers import (\n",
    "    ContextDataset,\n",
    "    StandardSupervisedDataset,\n",
    "    FixedLengthContextDataset,\n",
    "    RepeatedContextDataset,\n",
    ")\n",
    "from jaxl.models import load_config, load_model, get_model, get_activation\n",
    "from jaxl.plot_utils import set_size\n",
    "from jaxl.utils import parse_dict, get_device\n",
    "\n",
    "import _pickle as pickle\n",
    "import copy\n",
    "import jax\n",
    "import jax.random as jrandom\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision.datasets as torch_datasets\n",
    "\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from orbax.checkpoint import PyTreeCheckpointer, CheckpointManager\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cpu\"\n",
    "device = \"gpu:0\"\n",
    "get_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_width_pt = 750.0\n",
    "\n",
    "base_path = \"/home/bryanpu1/projects/icl/jaxl/\"\n",
    "data_path = os.path.join(base_path, \"data\")\n",
    "log_path = os.path.join(base_path, \"jaxl/logs\")\n",
    "project_name = \"icl-omniglot\"\n",
    "run_name = (\n",
    "    \"resnet-no_bn-context_len_16-num_blocks_8-02-13-24_01_00_39-26d7ced5-d8ff-4ab3-8cfb-2cc30b069a1d\"\n",
    ")\n",
    "\n",
    "learner_path = os.path.join(\n",
    "    log_path,\n",
    "    project_name,\n",
    "    run_name,\n",
    ")\n",
    "\n",
    "exp_name = \"-\".join(run_name.split(\"-\")[:-8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict, config = load_config(learner_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(\n",
    "    config.learner_config.dataset_config,\n",
    "    config.learner_config.seeds.data_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, model = load_model(\n",
    "    train_dataset.input_dim, train_dataset.output_dim, learner_path, -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len = config.model_config.num_contexts\n",
    "num_samples_per_task = train_dataset._dataset.sequence_length - 1\n",
    "sequence_length = train_dataset._dataset.sequence_length\n",
    "num_tasks = 100\n",
    "num_workers = 4\n",
    "\n",
    "print(num_samples_per_task, num_tasks, sequence_length, context_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=num_samples_per_task,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_labels(data_loader, num_tasks):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "\n",
    "    for batch_i, samples in enumerate(data_loader):\n",
    "        if batch_i >= num_tasks:\n",
    "            break\n",
    "\n",
    "        (context_inputs, context_outputs, queries, one_hot_labels) = samples\n",
    "\n",
    "        outputs, _, _ = model.forward(\n",
    "            params[CONST_MODEL_DICT][CONST_MODEL],\n",
    "            queries.numpy(),\n",
    "            {\n",
    "                CONST_CONTEXT_INPUT: context_inputs.numpy(),\n",
    "                CONST_CONTEXT_OUTPUT: context_outputs.numpy(),\n",
    "            },\n",
    "            eval=True,\n",
    "        )\n",
    "        preds = np.argmax(outputs, axis=-1)\n",
    "        labels = np.argmax(one_hot_labels, axis=-1)\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "        all_outputs.append(outputs)\n",
    "\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return all_preds, all_labels, all_outputs\n",
    "\n",
    "\n",
    "def print_performance(\n",
    "    all_preds,\n",
    "    all_labels,\n",
    "    sequence_length,\n",
    "    context_len,\n",
    "    output_dim,\n",
    "):\n",
    "    conf_mat = confusion_matrix(all_labels, all_preds, labels=np.arange(output_dim))\n",
    "    acc = np.trace(conf_mat) / np.sum(conf_mat) * 100\n",
    "    print(\"Pretraining Accuracy: {}\".format(acc))\n",
    "\n",
    "    reshaped_preds = all_preds.reshape((-1, sequence_length - 1))\n",
    "    reshaped_labels = all_labels.reshape((-1, sequence_length - 1))\n",
    "    for curr_context_len in range(context_len):\n",
    "        if curr_context_len < context_len - 1:\n",
    "            curr_preds = reshaped_preds[:, curr_context_len]\n",
    "            curr_labels = reshaped_labels[:, curr_context_len]\n",
    "        else:\n",
    "            curr_preds = reshaped_preds[:, curr_context_len:]\n",
    "            curr_labels = reshaped_labels[:, curr_context_len:]\n",
    "\n",
    "        curr_preds = curr_preds.reshape(-1)\n",
    "        curr_labels = curr_labels.reshape(-1)\n",
    "\n",
    "        curr_conf_mat = confusion_matrix(\n",
    "            curr_labels, curr_preds, labels=np.arange(output_dim)\n",
    "        )\n",
    "        curr_acc = np.trace(curr_conf_mat) / np.sum(curr_conf_mat) * 100\n",
    "        print(\n",
    "            \"Pretraining Accuracy with Context Length {} (Num Samples: {}): {}\".format(\n",
    "                curr_context_len + 1, np.sum(curr_conf_mat), curr_acc\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_i in range(2):\n",
    "    ci, co, q, l = train_dataset[\n",
    "        task_i * num_samples_per_task + num_samples_per_task - 1\n",
    "    ]\n",
    "\n",
    "    nrows = 2\n",
    "    ncols = 8\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows,\n",
    "        ncols,\n",
    "        figsize=set_size(doc_width_pt, 0.95, (nrows, ncols), False),\n",
    "        layout=\"constrained\",\n",
    "    )\n",
    "\n",
    "    for idx, (img, label) in enumerate(zip(ci, co)):\n",
    "        axes[idx // ncols, idx % ncols].imshow(img[0])\n",
    "        axes[idx // ncols, idx % ncols].set_title(np.argmax(label))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.imshow(q[0][0])\n",
    "    plt.title(np.argmax(l))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Training Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds, train_labels, train_outputs = get_preds_labels(train_loader, num_tasks)\n",
    "pickle.dump(\n",
    "    [train_preds, train_labels, train_outputs],\n",
    "    open(\"train_prediction_result.pkl\", \"wb\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance(\n",
    "    train_preds,\n",
    "    train_labels,\n",
    "    sequence_length,\n",
    "    context_len,\n",
    "    train_dataset.output_dim[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-distribution Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_in_dist_test_tasks = 30\n",
    "in_dist_test_data_seed = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.learner_config.dataset_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dist_test_config_dict = copy.deepcopy(\n",
    "    config_dict[\"learner_config\"][\"dataset_config\"]\n",
    ")\n",
    "in_dist_test_config_dict[\"dataset_kwargs\"][\"num_sequences\"] = num_in_dist_test_tasks\n",
    "in_dist_test_config = parse_dict(in_dist_test_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dist_test_dataset = get_dataset(\n",
    "    in_dist_test_config,\n",
    "    in_dist_test_data_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dist_test_loader = DataLoader(\n",
    "    in_dist_test_dataset,\n",
    "    batch_size=num_samples_per_task,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dist_test_preds, in_dist_test_labels, in_dist_test_outputs = get_preds_labels(\n",
    "    in_dist_test_loader, num_in_dist_test_tasks\n",
    ")\n",
    "pickle.dump(\n",
    "    [in_dist_test_preds, in_dist_test_labels, in_dist_test_outputs],\n",
    "    open(\"in_dist_test_prediction_result.pkl\", \"wb\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance(\n",
    "    in_dist_test_preds,\n",
    "    in_dist_test_labels,\n",
    "    sequence_length,\n",
    "    context_len,\n",
    "    in_dist_test_dataset.output_dim[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-class Test Data\n",
    "This checks out-of-class generalization (i.e. heldout classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ooc_test_tasks = 30\n",
    "ooc_test_data_seed = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ooc_test_config_dict = copy.deepcopy(config_dict[\"learner_config\"][\"dataset_config\"])\n",
    "ooc_test_config_dict[\"dataset_kwargs\"][\"split\"] = \"test\"\n",
    "ooc_test_config_dict[\"dataset_kwargs\"][\"num_sequences\"] = num_ooc_test_tasks\n",
    "ooc_test_config = parse_dict(ooc_test_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ooc_test_dataset = get_dataset(\n",
    "    ooc_test_config,\n",
    "    ooc_test_data_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ooc_test_loader = DataLoader(\n",
    "    ooc_test_dataset,\n",
    "    batch_size=num_samples_per_task,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ooc_test_preds, ooc_test_labels, ooc_test_outputs = get_preds_labels(\n",
    "    ooc_test_loader, num_ooc_test_tasks\n",
    ")\n",
    "pickle.dump(\n",
    "    [ooc_test_preds, ooc_test_labels, ooc_test_outputs],\n",
    "    open(\"ooc_test_prediction_result.pkl\", \"wb\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance(\n",
    "    ooc_test_preds,\n",
    "    ooc_test_labels,\n",
    "    sequence_length,\n",
    "    context_len,\n",
    "    ooc_test_dataset.output_dim[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relabel Test Data\n",
    "Maps the labels to a constrained subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_remap_test_tasks = 30\n",
    "remap_test_data_seed = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap_test_config_dict = copy.deepcopy(config_dict[\"learner_config\"][\"dataset_config\"])\n",
    "remap_test_config_dict[\"dataset_kwargs\"][\"split\"] = \"test\"\n",
    "remap_test_config_dict[\"dataset_kwargs\"][\"remap\"] = True\n",
    "remap_test_config_dict[\"dataset_kwargs\"][\"num_sequences\"] = num_remap_test_tasks\n",
    "remap_test_config = parse_dict(remap_test_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap_test_dataset = get_dataset(\n",
    "    remap_test_config,\n",
    "    remap_test_data_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap_test_loader = DataLoader(\n",
    "    remap_test_dataset,\n",
    "    batch_size=num_samples_per_task,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap_test_preds, remap_test_labels, remap_test_outputs = get_preds_labels(\n",
    "    remap_test_loader, num_remap_test_tasks\n",
    ")\n",
    "pickle.dump(\n",
    "    [remap_test_preds, remap_test_labels, remap_test_outputs],\n",
    "    open(\"remap_test_prediction_result.pkl\", \"wb\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance(\n",
    "    remap_test_preds,\n",
    "    remap_test_labels,\n",
    "    sequence_length,\n",
    "    context_len,\n",
    "    remap_test_dataset.output_dim[0],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
