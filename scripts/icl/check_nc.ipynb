{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Neural Collapse on the Input Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace\n",
    "from typing import Dict, Any\n",
    "\n",
    "import _pickle as pickle\n",
    "import argparse\n",
    "import copy\n",
    "import jax\n",
    "import numpy as np\n",
    "import os\n",
    "import timeit\n",
    "\n",
    "from jaxl.constants import *\n",
    "from jaxl.datasets import get_dataset\n",
    "from jaxl.models import load_config, iterate_models, get_model\n",
    "from jaxl.utils import parse_dict, get_device\n",
    "\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "# device = \"gpu:0\"\n",
    "get_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_path = \"/Users/chanb/research/personal/icl/saved_models/icl-omniglot/async-04-16-24_12_53_49-019260d2-241d-4285-b98e-d1d9fe05e929\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict, config = load_config(learner_path)\n",
    "\n",
    "if config.learner_config.dataset_config.dataset_kwargs.save_path.startswith(\"/home/bryanpu1\"):\n",
    "    config_dict[\"learner_config\"][\"dataset_config\"][\"dataset_kwargs\"][\"save_path\"] = \"/Users/chanb/research/personal/icl/data\"\n",
    "    config_dict[\"learner_config\"][\"dataset_config\"][\"dataset_kwargs\"][\"task_config\"][\"save_dir\"] = \"/Users/chanb/research/personal/icl/data/omniglot_icl\"\n",
    "    config_dict[\"learner_config\"][\"dataset_config\"][\"dataset_kwargs\"][\"task_config\"][\"num_sequences\"] = 320\n",
    "    config_dict[\"learner_config\"][\"dataset_config\"][\"num_workers\"] = 0\n",
    "    config = parse_dict(config_dict)\n",
    "\n",
    "train_dataset = get_dataset(\n",
    "    config.learner_config.dataset_config,\n",
    "    config.learner_config.seeds.data_seed,\n",
    ")\n",
    "\n",
    "context_len = config.model_config.num_contexts\n",
    "\n",
    "fixed_length = True\n",
    "if hasattr(config.learner_config.dataset_config, \"dataset_wrapper\"):\n",
    "    fixed_length = config.learner_config.dataset_config.dataset_wrapper.type in [\n",
    "        \"FixedLengthContextDataset\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_steps = []\n",
    "all_tokens = []\n",
    "for params, model, checkpoint_step in iterate_models(\n",
    "    train_dataset.input_dim, train_dataset.output_dim, learner_path\n",
    "):\n",
    "    dataset_loader = train_dataset.get_dataloader(config.learner_config)\n",
    "    checkpoint_steps.append(checkpoint_step)\n",
    "    for batch_i, data in enumerate(dataset_loader):\n",
    "        context_inputs = data[\"context_inputs\"]\n",
    "        context_outputs = data[\"context_outputs\"]\n",
    "        queries = data[\"queries\"]\n",
    "        one_hot_labels = data[\"outputs\"]\n",
    "\n",
    "        if hasattr(context_inputs, \"numpy\"):\n",
    "            context_inputs = context_inputs.numpy()\n",
    "            context_outputs = context_outputs.numpy()\n",
    "            queries = queries.numpy()\n",
    "            one_hot_labels = one_hot_labels.numpy()\n",
    "\n",
    "        tokens, _, _ = model.tokenize(\n",
    "            params[CONST_MODEL_DICT][CONST_MODEL],\n",
    "            queries,\n",
    "            {\n",
    "                CONST_CONTEXT_INPUT: context_inputs,\n",
    "                CONST_CONTEXT_OUTPUT: context_outputs,\n",
    "            },\n",
    "            eval=True,\n",
    "        )\n",
    "        all_tokens.append(tokens)\n",
    "\n",
    "all_tokens = np.concatenate(all_tokens, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
