{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ICL Model on the Omniglot Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxl.constants import *\n",
    "from jaxl.datasets import get_dataset\n",
    "from jaxl.datasets.wrappers import (\n",
    "    ContextDataset,\n",
    "    StandardSupervisedDataset,\n",
    "    FixedLengthContextDataset,\n",
    "    RepeatedContextDataset,\n",
    ")\n",
    "from jaxl.models import load_config, load_model, get_model, get_activation\n",
    "from jaxl.plot_utils import set_size\n",
    "from jaxl.utils import parse_dict, get_device\n",
    "\n",
    "import _pickle as pickle\n",
    "import copy\n",
    "import jax\n",
    "import jax.random as jrandom\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision.datasets as torch_datasets\n",
    "\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from orbax.checkpoint import PyTreeCheckpointer, CheckpointManager\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cpu\"\n",
    "device = \"gpu:0\"\n",
    "get_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_width_pt = 750.0\n",
    "\n",
    "base_path = \"/home/bryanpu1/projects/icl/jaxl/\"\n",
    "data_path = os.path.join(base_path, \"data\")\n",
    "log_path = os.path.join(base_path, \"jaxl/logs\")\n",
    "project_name = \"icl-omniglot\"\n",
    "# ablation_name = \"bursty_ablation\"\n",
    "ablation_name = \"bursty_ablation-fixed_length\"\n",
    "# ablation_name = \"\"\n",
    "run_name = (\n",
    "    # Input tokenizer\n",
    "    # \"cnn-context_len_16-num_blocks_8-02-12-24_23_14_55-b0846297-59fb-4e96-9eff-b4e05902f09c\"\n",
    "    # \"resnet-context_len_16-num_blocks_8-02-13-24_00_22_16-188aedab-34f0-4299-b830-4a62c45dcfe4\"\n",
    "    # \"resnet-no_bn-context_len_16-num_blocks_8-02-13-24_01_00_39-26d7ced5-d8ff-4ab3-8cfb-2cc30b069a1d\"\n",
    "\n",
    "    # Output tokenizer\n",
    "    # \"resnet-no_bn-context_len_16-num_blocks_8-02-13-24_14_16_38-8cb7114e-b2f7-4203-9db5-d059cb1a9651\"\n",
    "    # \"resnet-no_bn-frozen_output_tokenizer-context_len_16-num_blocks_8-02-14-24_15_57_52-f5648ff4-0443-4bd1-af75-fcc14e2146ff\"\n",
    "\n",
    "    # Noisy pretraining\n",
    "    # \"resnet-no_bn-frozen_output_tokenizer-noisy_pretraining-context_len_16-num_blocks_8-02-15-24_12_37_55-65f7ccfd-f504-46a7-a5e7-df355d08587b\"\n",
    "\n",
    "    # Noisy pretraining + 12 transformer blocks\n",
    "    # \"resnet-no_bn-frozen_output_tokenizer-noisy_pretraining-context_len_16-num_blocks_12-02-15-24_13_07_31-af0da6f1-43a8-4f6b-89f5-1f8117124861\"\n",
    "\n",
    "    # Burstiness experiments\n",
    "    # Variable length\n",
    "    # \"bursty_0.0-02-24-24_06_55_15-910c422e-36bb-4ff2-995a-1a7e13adf571\"\n",
    "    # \"bursty_0.5-02-24-24_06_55_11-af7e29d4-08f3-45c2-8fdd-7d052416f898\"\n",
    "    # \"bursty_1.0-02-24-24_06_55_07-3e790c78-6da7-4be3-ba93-1359ba67ab0a\"\n",
    "\n",
    "    # Fixed length\n",
    "    # \"bursty_0.0-02-25-24_15_22_20-b8aa457b-7a3a-498b-848c-0e7b819952e7\"\n",
    "    # \"bursty_0.5-02-25-24_15_22_11-483fa86e-36ee-410d-901d-ed1007f7881a\"\n",
    "    # \"bursty_0.5-02-26-24_18_11_59-a173f14d-bc0c-4990-9dea-1eedc118e85b\" # Longer training\n",
    "    # \"bursty_1.0-02-25-24_15_22_04-4ef72a13-fa8d-49cb-a5de-e09d36e3abaf\"\n",
    "\n",
    "    # Batch norm ablation\n",
    "    # \"bursty_0.0-batch_norm-02-28-24_12_45_42-5b3cd1ab-4768-4f74-9688-5be15bd4f9cd\"\n",
    "    \"bursty_0.5-batch_norm-02-27-24_20_56_01-8347c265-2e88-40c6-9cc0-30242f2b8f41\"\n",
    "    # \"bursty_0.5-num_blocks_12-batch_norm-02-27-24_20_29_48-7fcb7bcb-5ba1-459e-9129-3cc164b04860\"\n",
    "    # \"bursty_0.5-num_blocks_12-batch_norm-no_aug-02-27-24_20_37_07-04e2891c-636b-4610-b9f2-8efbebe2b1db\"\n",
    ")\n",
    "\n",
    "learner_path = os.path.join(\n",
    "    log_path,\n",
    "    project_name,\n",
    "    ablation_name,\n",
    "    run_name,\n",
    ")\n",
    "\n",
    "exp_name = \"-\".join(run_name.split(\"-\")[:-8])\n",
    "p_bursty = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict, config = load_config(learner_path)\n",
    "config_dict[\"learner_config\"][\"dataset_config\"][\"dataset_kwargs\"][\"task_config\"][\"p_bursty\"] = p_bursty\n",
    "fixed_length = config.learner_config.dataset_config.dataset_wrapper.type in [\"FixedLengthContextDataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(\n",
    "    config.learner_config.dataset_config,\n",
    "    config.learner_config.seeds.data_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, model = load_model(\n",
    "    train_dataset.input_dim, train_dataset.output_dim, learner_path, -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len = config.model_config.num_contexts\n",
    "num_samples_per_task = train_dataset._dataset.sequence_length - 1\n",
    "sequence_length = train_dataset._dataset.sequence_length\n",
    "num_tasks = 100\n",
    "num_workers = 4\n",
    "\n",
    "print(num_samples_per_task, num_tasks, sequence_length, context_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=num_samples_per_task,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_labels(data_loader, num_tasks, max_label=None):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "\n",
    "    for batch_i, samples in enumerate(data_loader):\n",
    "        if batch_i >= num_tasks:\n",
    "            break\n",
    "\n",
    "        (context_inputs, context_outputs, queries, one_hot_labels) = samples\n",
    "\n",
    "        train_outputs, _, train_updates = model.forward(\n",
    "            params[CONST_MODEL_DICT][CONST_MODEL],\n",
    "            queries.numpy(),\n",
    "            {\n",
    "                CONST_CONTEXT_INPUT: context_inputs.numpy(),\n",
    "                CONST_CONTEXT_OUTPUT: context_outputs.numpy(),\n",
    "            },\n",
    "            eval=False,\n",
    "        )\n",
    "\n",
    "        outputs, _, updates = model.forward(\n",
    "            params[CONST_MODEL_DICT][CONST_MODEL],\n",
    "            queries.numpy(),\n",
    "            {\n",
    "                CONST_CONTEXT_INPUT: context_inputs.numpy(),\n",
    "                CONST_CONTEXT_OUTPUT: context_outputs.numpy(),\n",
    "            },\n",
    "            eval=True,\n",
    "        )\n",
    "        return train_outputs, train_updates, outputs, updates\n",
    "        if max_label is None:\n",
    "            preds = np.argmax(outputs, axis=-1)\n",
    "        else:\n",
    "            preds = np.argmax(outputs[..., :max_label], axis=-1)\n",
    "        labels = np.argmax(one_hot_labels, axis=-1)\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "        all_outputs.append(outputs)\n",
    "\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return all_preds, all_labels, all_outputs\n",
    "\n",
    "\n",
    "def print_performance(\n",
    "    all_preds,\n",
    "    all_labels,\n",
    "    sequence_length,\n",
    "    context_len,\n",
    "    output_dim,\n",
    "    fixed_length = False,\n",
    "):\n",
    "    conf_mat = confusion_matrix(all_labels, all_preds, labels=np.arange(output_dim))\n",
    "    acc = np.trace(conf_mat) / np.sum(conf_mat) * 100\n",
    "    print(\"Pretraining Accuracy: {}\".format(acc))\n",
    "\n",
    "    if not fixed_length:\n",
    "        reshaped_preds = all_preds.reshape((-1, sequence_length - 1))\n",
    "        reshaped_labels = all_labels.reshape((-1, sequence_length - 1))\n",
    "        for curr_context_len in range(context_len):\n",
    "            if curr_context_len < context_len - 1:\n",
    "                curr_preds = reshaped_preds[:, curr_context_len]\n",
    "                curr_labels = reshaped_labels[:, curr_context_len]\n",
    "            else:\n",
    "                curr_preds = reshaped_preds[:, curr_context_len:]\n",
    "                curr_labels = reshaped_labels[:, curr_context_len:]\n",
    "\n",
    "            curr_preds = curr_preds.reshape(-1)\n",
    "            curr_labels = curr_labels.reshape(-1)\n",
    "\n",
    "            curr_conf_mat = confusion_matrix(\n",
    "                curr_labels, curr_preds, labels=np.arange(output_dim)\n",
    "            )\n",
    "            curr_acc = np.trace(curr_conf_mat) / np.sum(curr_conf_mat) * 100\n",
    "            print(\n",
    "                \"Pretraining Accuracy with Context Length {} (Num Samples: {}): {}\".format(\n",
    "                    curr_context_len + 1, np.sum(curr_conf_mat), curr_acc\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_outputs, train_updates, outputs, updates = get_preds_labels(train_loader, num_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = True\n",
    "if check:\n",
    "    for task_i in range(2):\n",
    "        ci, co, q, l = train_dataset[\n",
    "            task_i * num_samples_per_task + num_samples_per_task - 1\n",
    "        ]\n",
    "\n",
    "        nrows = 2\n",
    "        ncols = 8\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows,\n",
    "            ncols + 1,\n",
    "            figsize=set_size(doc_width_pt, 0.95, (nrows, ncols), False),\n",
    "            layout=\"constrained\",\n",
    "        )\n",
    "\n",
    "        for idx, (img, label) in enumerate(zip(ci, co)):\n",
    "            axes[idx // ncols, idx % ncols].imshow(img[0])\n",
    "            axes[idx // ncols, idx % ncols].set_title(np.argmax(label))\n",
    "            axes[idx // ncols, idx % ncols].axis('off')\n",
    "        axes[0, -1].axis('off')\n",
    "        axes[1, -1].axis('off')\n",
    "        axes[1, -1].imshow(q[0, 0])\n",
    "        axes[1, -1].set_title(np.argmax(l, axis=-1))\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Training Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds, train_labels, train_outputs = get_preds_labels(train_loader, num_tasks)\n",
    "pickle.dump(\n",
    "    [train_preds, train_labels, train_outputs],\n",
    "    open(\"train_prediction_result.pkl\", \"wb\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance(\n",
    "    train_preds,\n",
    "    train_labels,\n",
    "    sequence_length,\n",
    "    context_len,\n",
    "    train_dataset.output_dim[0],\n",
    "    fixed_length=fixed_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-distribution Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_in_dist_test_tasks = 30\n",
    "in_dist_test_data_seed = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.learner_config.dataset_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dist_test_config_dict = copy.deepcopy(\n",
    "    config_dict[\"learner_config\"][\"dataset_config\"]\n",
    ")\n",
    "in_dist_test_config_dict[\"dataset_kwargs\"][\"num_sequences\"] = num_in_dist_test_tasks\n",
    "in_dist_test_config_dict[\"dataset_kwargs\"][\"task_config\"][\"augmentation\"] = True\n",
    "in_dist_test_config_dict[\"dataset_kwargs\"][\"task_config\"][\"noise_scale\"] = 0.1\n",
    "in_dist_test_config = parse_dict(in_dist_test_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dist_test_dataset = get_dataset(\n",
    "    in_dist_test_config,\n",
    "    in_dist_test_data_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = True\n",
    "if check:\n",
    "    for task_i in range(2):\n",
    "        ci, co, q, l = in_dist_test_dataset[\n",
    "            task_i * num_samples_per_task + num_samples_per_task - 1\n",
    "        ]\n",
    "\n",
    "        nrows = 2\n",
    "        ncols = 8\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows,\n",
    "            ncols + 1,\n",
    "            figsize=set_size(doc_width_pt, 0.95, (nrows, ncols), False),\n",
    "            layout=\"constrained\",\n",
    "        )\n",
    "\n",
    "        for idx, (img, label) in enumerate(zip(ci, co)):\n",
    "            axes[idx // ncols, idx % ncols].imshow(img[0])\n",
    "            axes[idx // ncols, idx % ncols].set_title(np.argmax(label))\n",
    "            axes[idx // ncols, idx % ncols].axis('off')\n",
    "        axes[0, -1].axis('off')\n",
    "        axes[1, -1].axis('off')\n",
    "        axes[1, -1].imshow(q[0, 0])\n",
    "        axes[1, -1].set_title(np.argmax(l, axis=-1))\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dist_test_loader = DataLoader(\n",
    "    in_dist_test_dataset,\n",
    "    batch_size=num_samples_per_task,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dist_test_preds, in_dist_test_labels, in_dist_test_outputs = get_preds_labels(\n",
    "    in_dist_test_loader, num_in_dist_test_tasks\n",
    ")\n",
    "pickle.dump(\n",
    "    [in_dist_test_preds, in_dist_test_labels, in_dist_test_outputs],\n",
    "    open(\"in_dist_test_prediction_result.pkl\", \"wb\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance(\n",
    "    in_dist_test_preds,\n",
    "    in_dist_test_labels,\n",
    "    sequence_length,\n",
    "    context_len,\n",
    "    in_dist_test_dataset.output_dim[0],\n",
    "    fixed_length=fixed_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relabel Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_remap_train_tasks = 30\n",
    "remap_train_data_seed = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap_train_config_dict = copy.deepcopy(config_dict[\"learner_config\"][\"dataset_config\"])\n",
    "remap_train_config_dict[\"dataset_kwargs\"][\"train\"] = True\n",
    "remap_train_config_dict[\"dataset_kwargs\"][\"remap\"] = True\n",
    "remap_train_config_dict[\"dataset_kwargs\"][\"num_sequences\"] = num_remap_train_tasks\n",
    "remap_train_config = parse_dict(remap_train_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap_train_dataset = get_dataset(\n",
    "    remap_train_config,\n",
    "    remap_train_data_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = True\n",
    "if check:\n",
    "    for task_i in range(2):\n",
    "        ci, co, q, l = remap_train_dataset[\n",
    "            task_i * num_samples_per_task + num_samples_per_task - 1\n",
    "        ]\n",
    "\n",
    "        nrows = 2\n",
    "        ncols = 8\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows,\n",
    "            ncols + 1,\n",
    "            figsize=set_size(doc_width_pt, 0.95, (nrows, ncols), False),\n",
    "            layout=\"constrained\",\n",
    "        )\n",
    "\n",
    "        for idx, (img, label) in enumerate(zip(ci, co)):\n",
    "            axes[idx // ncols, idx % ncols].imshow(img[0])\n",
    "            axes[idx // ncols, idx % ncols].set_title(np.argmax(label))\n",
    "            axes[idx // ncols, idx % ncols].axis('off')\n",
    "        axes[0, -1].axis('off')\n",
    "        axes[1, -1].axis('off')\n",
    "        axes[1, -1].imshow(q[0, 0])\n",
    "        axes[1, -1].set_title(np.argmax(l, axis=-1))\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap_train_loader = DataLoader(\n",
    "    remap_train_dataset,\n",
    "    batch_size=num_samples_per_task,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap_train_preds, remap_train_labels, remap_train_outputs = get_preds_labels(\n",
    "    remap_train_loader, num_remap_train_tasks, max_label=2,\n",
    ")\n",
    "pickle.dump(\n",
    "    [remap_train_preds, remap_train_labels, remap_train_outputs],\n",
    "    open(\"remap_train_prediction_result.pkl\", \"wb\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance(\n",
    "    remap_train_preds,\n",
    "    remap_train_labels,\n",
    "    sequence_length,\n",
    "    context_len,\n",
    "    remap_train_dataset.output_dim[0],\n",
    "    fixed_length=fixed_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-class Test Data\n",
    "This checks out-of-class generalization (i.e. heldout classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ooc_test_tasks = 30\n",
    "ooc_test_data_seed = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ooc_test_config_dict = copy.deepcopy(config_dict[\"learner_config\"][\"dataset_config\"])\n",
    "ooc_test_config_dict[\"dataset_kwargs\"][\"train\"] = False\n",
    "ooc_test_config_dict[\"dataset_kwargs\"][\"remap\"] = False\n",
    "ooc_test_config_dict[\"dataset_kwargs\"][\"num_sequences\"] = num_ooc_test_tasks\n",
    "ooc_test_config = parse_dict(ooc_test_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ooc_test_dataset = get_dataset(\n",
    "    ooc_test_config,\n",
    "    ooc_test_data_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check = True\n",
    "if check:\n",
    "    for task_i in range(2):\n",
    "        ci, co, q, l = ooc_test_dataset[\n",
    "            task_i * num_samples_per_task + num_samples_per_task - 1\n",
    "        ]\n",
    "\n",
    "        nrows = 2\n",
    "        ncols = 8\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows,\n",
    "            ncols + 1,\n",
    "            figsize=set_size(doc_width_pt, 0.95, (nrows, ncols), False),\n",
    "            layout=\"constrained\",\n",
    "        )\n",
    "\n",
    "        for idx, (img, label) in enumerate(zip(ci, co)):\n",
    "            axes[idx // ncols, idx % ncols].imshow(img[0])\n",
    "            axes[idx // ncols, idx % ncols].set_title(np.argmax(label))\n",
    "            axes[idx // ncols, idx % ncols].axis('off')\n",
    "        axes[0, -1].axis('off')\n",
    "        axes[1, -1].axis('off')\n",
    "        axes[1, -1].imshow(q[0, 0])\n",
    "        axes[1, -1].set_title(np.argmax(l, axis=-1))\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ooc_test_loader = DataLoader(\n",
    "    ooc_test_dataset,\n",
    "    batch_size=num_samples_per_task,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ooc_test_preds, ooc_test_labels, ooc_test_outputs = get_preds_labels(\n",
    "    ooc_test_loader, num_ooc_test_tasks\n",
    ")\n",
    "pickle.dump(\n",
    "    [ooc_test_preds, ooc_test_labels, ooc_test_outputs],\n",
    "    open(\"ooc_test_prediction_result.pkl\", \"wb\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance(\n",
    "    ooc_test_preds,\n",
    "    ooc_test_labels,\n",
    "    sequence_length,\n",
    "    context_len,\n",
    "    ooc_test_dataset.output_dim[0],\n",
    "    fixed_length=fixed_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relabel Test Data\n",
    "Maps the labels to a constrained subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_remap_test_tasks = 30\n",
    "remap_test_data_seed = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap_test_config_dict = copy.deepcopy(config_dict[\"learner_config\"][\"dataset_config\"])\n",
    "remap_test_config_dict[\"dataset_kwargs\"][\"train\"] = False\n",
    "remap_test_config_dict[\"dataset_kwargs\"][\"remap\"] = True\n",
    "remap_test_config_dict[\"dataset_kwargs\"][\"num_sequences\"] = num_remap_test_tasks\n",
    "remap_test_config = parse_dict(remap_test_config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap_test_dataset = get_dataset(\n",
    "    remap_test_config,\n",
    "    remap_test_data_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap_test_loader = DataLoader(\n",
    "    remap_test_dataset,\n",
    "    batch_size=num_samples_per_task,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap_test_preds, remap_test_labels, remap_test_outputs = get_preds_labels(\n",
    "    remap_test_loader, num_remap_test_tasks, max_label=2\n",
    ")\n",
    "pickle.dump(\n",
    "    [remap_test_preds, remap_test_labels, remap_test_outputs],\n",
    "    open(\"remap_test_prediction_result.pkl\", \"wb\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_performance(\n",
    "    remap_test_preds,\n",
    "    remap_test_labels,\n",
    "    sequence_length,\n",
    "    context_len,\n",
    "    remap_test_dataset.output_dim[0],\n",
    "    fixed_length=fixed_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap_test_preds, remap_test_labels, remap_test_outputs = get_preds_labels(\n",
    "    remap_test_loader, num_remap_test_tasks\n",
    ")\n",
    "pickle.dump(\n",
    "    [remap_test_preds, remap_test_labels, remap_test_outputs],\n",
    "    open(\"remap_test_prediction_result-unconstrained.pkl\", \"wb\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print_performance(\n",
    "    remap_test_preds,\n",
    "    remap_test_labels,\n",
    "    sequence_length,\n",
    "    context_len,\n",
    "    remap_test_dataset.output_dim[0],\n",
    "    fixed_length=fixed_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
