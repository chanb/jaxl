{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxl.constants import *\n",
    "from jaxl.datasets import get_dataset\n",
    "from jaxl.learning_utils import get_learner\n",
    "from jaxl.plot_utils import set_size\n",
    "from jaxl.utils import parse_dict\n",
    "\n",
    "import jax\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import optax\n",
    "import os\n",
    "\n",
    "from orbax.checkpoint import PyTreeCheckpointer, CheckpointManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/Users/chanb/research/personal/jaxl/jaxl\"\n",
    "learner_path = \"/Users/chanb/research/personal/jaxl/jaxl/logs/icl-noiseless-no_bias-2d_linear-active_dim_2-context_len_20/gpt-pos_enc-09-09-23_19_18_47-0b5d21fd-3b77-42e3-93ed-4e62dfbe1321\"\n",
    "test_dataset_seed = 999\n",
    "sequence_len = 80\n",
    "num_tasks = 10\n",
    "\n",
    "# For plotting\n",
    "doc_width_pt = 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(learner_path, \"config.json\")\n",
    "with open(config_path, \"r\") as f:\n",
    "    config_dict = json.load(f)\n",
    "    config = parse_dict(config_dict)\n",
    "\n",
    "learner = get_learner(\n",
    "    config.learner_config, config.model_config, config.optimizer_config\n",
    ")\n",
    "\n",
    "checkpoint_manager = CheckpointManager(\n",
    "    os.path.join(learner_path, \"models\"),\n",
    "    PyTreeCheckpointer(),\n",
    ")\n",
    "\n",
    "params = checkpoint_manager.restore(checkpoint_manager.latest_step())\n",
    "params[CONST_MODEL_DICT][CONST_MODEL][CONST_POSITIONAL_ENCODING] = dict()\n",
    "llm_model = learner._model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.learner_config.dataset_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_range = [-1.0, 1.0]\n",
    "test_config = vars(config.learner_config.dataset_config)\n",
    "test_config[\"dataset_kwargs\"] = vars(test_config[\"dataset_kwargs\"])\n",
    "test_config[\"dataset_kwargs\"][\"num_sequences\"] = num_tasks\n",
    "test_config[\"dataset_kwargs\"][\"sequence_length\"] = sequence_len + 1\n",
    "test_config[\"dataset_kwargs\"][\"params_bound\"] = [-0.5, 0.5]\n",
    "test_config[\"dataset_kwargs\"][\"inputs_range\"] = input_range\n",
    "ns_test_config = parse_dict(test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = get_dataset(ns_test_config, seed=test_dataset_seed)\n",
    "unwrapped_dataset = test_dataset._dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[0][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unwrapped_dataset._inputs.shape, unwrapped_dataset._targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len = config.learner_config.dataset_config.dataset_wrapper.kwargs.context_len\n",
    "\n",
    "\n",
    "def get_result(dataset, task_i, context_len):\n",
    "    context_inputs, context_outputs = [], []\n",
    "    for context_i in range(context_len):\n",
    "        context_inputs.append(dataset._inputs[task_i, context_i])\n",
    "        context_outputs.append(dataset._targets[task_i, context_i])\n",
    "    context_inputs = np.stack(context_inputs)\n",
    "    context_outputs = np.stack(context_outputs)\n",
    "\n",
    "    queries = dataset._inputs[task_i, context_len:]\n",
    "    outputs = dataset._targets[task_i, context_len:]\n",
    "\n",
    "    preds, _ = jax.vmap(llm_model.forward, in_axes=[None, 0, None])(\n",
    "        params[CONST_MODEL_DICT][CONST_MODEL],\n",
    "        queries[:, None, None],\n",
    "        {\n",
    "            CONST_CONTEXT_INPUT: context_inputs[None, :],\n",
    "            CONST_CONTEXT_OUTPUT: context_outputs[None, :],\n",
    "        },\n",
    "    )\n",
    "    return queries, preds, outputs, context_inputs, context_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_loss(logits, y_one_hot):\n",
    "    return np.mean(optax.softmax_cross_entropy(logits, y_one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for task_i in range(num_tasks):\n",
    "    queries, preds, outputs, context_inputs, context_outputs = get_result(\n",
    "        unwrapped_dataset, task_i, context_len\n",
    "    )\n",
    "    preds = preds[:, 0]\n",
    "    loss = ce_loss(preds, outputs)\n",
    "    res.setdefault(task_i, {})\n",
    "    res[task_i][\"data\"] = {\n",
    "        \"context_inputs\": context_inputs,\n",
    "        \"context_outputs\": context_outputs,\n",
    "        \"queries\": queries,\n",
    "        \"outputs\": outputs,\n",
    "    }\n",
    "    res[task_i][\"llm\"] = preds\n",
    "\n",
    "    gt = test_dataset.params[task_i]\n",
    "    res[task_i][\"gt\"] = -np.array(input_range) * gt[1] / gt[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_regs = [1e-2, 1e-1, 5e-1, 1.0, 2.0, 10.0, 100.0, 1000.0]\n",
    "lr_regs = [1e-2, 1e-1, 5e-1, 1.0, 2.0, 10.0, 100.0, 1000.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_svm(inputs, outputs, reg_coef):\n",
    "    svm = make_pipeline(\n",
    "        LinearSVC(\n",
    "            dual=\"auto\",\n",
    "            loss=\"hinge\",\n",
    "            random_state=0,\n",
    "            tol=1e-7,\n",
    "            C=reg_coef,\n",
    "        ),\n",
    "    )\n",
    "    svm.fit(inputs, np.argmax(outputs, axis=1))\n",
    "    return svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_i in range(num_tasks):\n",
    "    svms = {}\n",
    "\n",
    "    ncols = 4\n",
    "    nrows = math.ceil(len(svm_regs) / ncols)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows,\n",
    "        ncols,\n",
    "        figsize=set_size(doc_width_pt, 0.95, (nrows, ncols), False),\n",
    "        layout=\"constrained\",\n",
    "    )\n",
    "\n",
    "    context_inputs = res[task_i][\"data\"][\"context_inputs\"]\n",
    "    context_outputs = res[task_i][\"data\"][\"context_outputs\"]\n",
    "\n",
    "    for idx, svm_reg in enumerate(svm_regs):\n",
    "        if nrows == 1:\n",
    "            ax = axes[idx]\n",
    "        else:\n",
    "            ax = axes[idx // ncols, idx % ncols]\n",
    "\n",
    "        svms[svm_reg] = make_svm(context_inputs, context_outputs, svm_reg)\n",
    "\n",
    "        decision_function = svms[svm_reg].decision_function(context_inputs)\n",
    "        support_vector_indices = np.where(np.abs(decision_function) <= 1)[0]\n",
    "        support_vectors = context_inputs[support_vector_indices]\n",
    "\n",
    "        ax.plot(\n",
    "            np.array(input_range),\n",
    "            res[task_i][\"gt\"],\n",
    "            label=\"Ground truth\" if idx == 0 else \"\",\n",
    "            color=\"red\",\n",
    "            alpha=0.3,\n",
    "        )\n",
    "\n",
    "        ax.scatter(\n",
    "            context_inputs[:, 0],\n",
    "            context_inputs[:, 1],\n",
    "            c=context_outputs[:, -1],\n",
    "            s=30,\n",
    "            cmap=plt.cm.Paired,\n",
    "        )\n",
    "        DecisionBoundaryDisplay.from_estimator(\n",
    "            svms[svm_reg],\n",
    "            context_inputs,\n",
    "            ax=ax,\n",
    "            grid_resolution=50,\n",
    "            plot_method=\"contour\",\n",
    "            colors=\"k\",\n",
    "            levels=[-1, 0, 1],\n",
    "            alpha=0.5,\n",
    "            linestyles=[\"--\", \"-\", \"--\"],\n",
    "        )\n",
    "        ax.scatter(\n",
    "            support_vectors[:, 0],\n",
    "            support_vectors[:, 1],\n",
    "            s=100,\n",
    "            linewidth=1,\n",
    "            facecolors=\"none\",\n",
    "            edgecolors=\"k\",\n",
    "        )\n",
    "        ax.set_xlim(input_range[0], input_range[1])\n",
    "        ax.set_ylim(input_range[0], input_range[1])\n",
    "        ax.set_title(f\"Reg. Coef.: {svm_reg}\")\n",
    "\n",
    "    fig.legend(\n",
    "        bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "        loc=\"lower center\",\n",
    "        ncols=3,\n",
    "        borderaxespad=0.0,\n",
    "        frameon=True,\n",
    "        fontsize=\"8\",\n",
    "    )\n",
    "    fig.supxlabel(\"$x_1$\")\n",
    "    fig.supylabel(\"$x_2$\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    res[task_i][\"svm\"] = svms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lr(inputs, outputs, penalty, reg_coef):\n",
    "    logistic_regression = make_pipeline(LogisticRegression(penalty=penalty, C=reg_coef))\n",
    "    logistic_regression.fit(inputs, np.argmax(outputs, axis=1))\n",
    "    return logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_i in range(num_tasks):\n",
    "    lrs = {}\n",
    "\n",
    "    ncols = 4\n",
    "    nrows = math.ceil(len(lr_regs) / ncols)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows,\n",
    "        ncols,\n",
    "        figsize=set_size(doc_width_pt, 0.95, (nrows, ncols), False),\n",
    "        layout=\"constrained\",\n",
    "    )\n",
    "\n",
    "    context_inputs = res[task_i][\"data\"][\"context_inputs\"]\n",
    "    context_outputs = res[task_i][\"data\"][\"context_outputs\"]\n",
    "\n",
    "    for idx, lr_reg in enumerate(lr_regs):\n",
    "        if nrows == 1:\n",
    "            ax = axes[idx]\n",
    "        else:\n",
    "            ax = axes[idx // ncols, idx % ncols]\n",
    "\n",
    "        ax.plot(\n",
    "            np.array(input_range),\n",
    "            res[task_i][\"gt\"],\n",
    "            label=\"Ground truth\" if idx == 0 else \"\",\n",
    "            color=\"red\",\n",
    "            alpha=0.3,\n",
    "        )\n",
    "\n",
    "        lrs[lr_reg] = make_lr(context_inputs, context_outputs, \"l2\", lr_reg)\n",
    "        ax.scatter(\n",
    "            context_inputs[:, 0],\n",
    "            context_inputs[:, 1],\n",
    "            c=context_outputs[:, -1],\n",
    "            s=30,\n",
    "            cmap=plt.cm.Paired,\n",
    "        )\n",
    "        DecisionBoundaryDisplay.from_estimator(\n",
    "            lrs[lr_reg],\n",
    "            context_inputs,\n",
    "            ax=ax,\n",
    "            grid_resolution=50,\n",
    "            plot_method=\"contour\",\n",
    "            colors=\"k\",\n",
    "            levels=[0],\n",
    "            alpha=0.5,\n",
    "            linestyles=[\"-\"],\n",
    "        )\n",
    "        ax.set_xlim(input_range[0], input_range[1])\n",
    "        ax.set_ylim(input_range[0], input_range[1])\n",
    "        ax.set_title(f\"Reg. Coef.: {lr_reg}\")\n",
    "\n",
    "    fig.legend(\n",
    "        bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "        loc=\"lower center\",\n",
    "        ncols=3,\n",
    "        borderaxespad=0.0,\n",
    "        frameon=True,\n",
    "        fontsize=\"8\",\n",
    "    )\n",
    "    fig.supxlabel(\"$x_1$\")\n",
    "    fig.supylabel(\"$x_2$\")\n",
    "    plt.tight_layout()\n",
    "    res[task_i][\"lr\"] = lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICL Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds = {}\n",
    "num_models = len(lr_regs) + len(svm_regs)\n",
    "\n",
    "delta = 0.01\n",
    "xs_grid = np.arange(-1.0, 1.0 + delta, delta)\n",
    "test_queries = np.stack(np.meshgrid(xs_grid, xs_grid)).reshape((2, -1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_i in range(num_tasks):\n",
    "    context_inputs = res[task_i][\"data\"][\"context_inputs\"]\n",
    "    context_outputs = res[task_i][\"data\"][\"context_outputs\"]\n",
    "\n",
    "    model_preds.setdefault(task_i, {})\n",
    "    llm_preds, _ = jax.vmap(llm_model.forward, in_axes=[None, 0, None])(\n",
    "        params[CONST_MODEL_DICT][CONST_MODEL],\n",
    "        test_queries[:, None, None],\n",
    "        {\n",
    "            CONST_CONTEXT_INPUT: context_inputs[None, :],\n",
    "            CONST_CONTEXT_OUTPUT: context_outputs[None, :],\n",
    "        },\n",
    "    )\n",
    "    llm_preds = llm_preds[:, 0]\n",
    "    model_preds[task_i][\"llm\"] = llm_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_i in range(num_tasks):\n",
    "    ncols = 4\n",
    "    nrows = math.ceil(num_models / ncols)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows,\n",
    "        ncols,\n",
    "        figsize=set_size(doc_width_pt, 0.95, (nrows, ncols), False),\n",
    "        layout=\"constrained\",\n",
    "    )\n",
    "\n",
    "    model_classes = {\n",
    "        \"SVM\": res[task_i][\"svm\"],\n",
    "        \"LR\": res[task_i][\"lr\"],\n",
    "    }\n",
    "\n",
    "    llm_preds = model_preds[task_i][\"llm\"]\n",
    "    llm_pred_labels = np.argmax(llm_preds, axis=-1)\n",
    "\n",
    "    model_i = 0\n",
    "    for model_class, models in model_classes.items():\n",
    "        for reg_coef, model in models.items():\n",
    "            model_out = (\n",
    "                -(np.array(input_range) * model[0].coef_[0, 0] + model[0].intercept_[0])\n",
    "                / model[0].coef_[0, 1]\n",
    "            )\n",
    "\n",
    "            if nrows == 1:\n",
    "                ax = axes[model_i]\n",
    "            else:\n",
    "                ax = axes[model_i // ncols, model_i % ncols]\n",
    "\n",
    "            for possible_label in [0, 1]:\n",
    "                idxes = np.where(llm_pred_labels == possible_label)\n",
    "                ax.scatter(\n",
    "                    test_queries[idxes][:, 0],\n",
    "                    test_queries[idxes][:, 1],\n",
    "                    label=f\"{possible_label}\" if model_i == 0 else \"\",\n",
    "                    s=5,\n",
    "                )\n",
    "\n",
    "            ax.plot(\n",
    "                np.array(input_range),\n",
    "                res[task_i][\"gt\"],\n",
    "                label=\"Ground truth\" if model_i == 0 else \"\",\n",
    "                color=\"black\",\n",
    "                linewidth=1,\n",
    "            )\n",
    "\n",
    "            ax.plot(np.array(input_range), model_out, color=\"blue\", linewidth=1, label=\"Comparator\" if model_i == 0 else \"\")\n",
    "\n",
    "            ax.set_xlim(input_range[0], input_range[1])\n",
    "            ax.set_ylim(input_range[0], input_range[1])\n",
    "            ax.set_title(f\"{model_class} {reg_coef}\")\n",
    "            model_i += 1\n",
    "\n",
    "\n",
    "    fig.legend(\n",
    "        bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "        loc=\"lower center\",\n",
    "        ncols=3,\n",
    "        borderaxespad=0.0,\n",
    "        frameon=True,\n",
    "        fontsize=\"8\",\n",
    "    )\n",
    "    fig.supxlabel(\"$x_1$\")\n",
    "    fig.supylabel(\"$x_2$\")\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean 0-1 Prediction Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {}\n",
    "\n",
    "for task_i in num_tasks:\n",
    "    model_classes = {\n",
    "        \"SVM\": res[task_i][\"svm\"],\n",
    "        \"LR\": res[task_i][\"lr\"],\n",
    "    }\n",
    "\n",
    "    for model_class, models in model_classes.items():\n",
    "        for reg_coef, model in models.items():\n",
    "            losses.setdefault((model_class, reg_coef), [])\n",
    "            losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
