{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate ICL Model on the MNIST Classification Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxl.constants import *\n",
    "from jaxl.datasets import get_dataset\n",
    "from jaxl.datasets.wrappers import (\n",
    "    ContextDataset,\n",
    "    StandardSupervisedDataset,\n",
    "    FixedLengthContextDataset,\n",
    "    RepeatedContextDataset,\n",
    ")\n",
    "from jaxl.models import load_config, load_model, get_model, get_activation\n",
    "from jaxl.plot_utils import set_size\n",
    "from jaxl.utils import parse_dict, get_device\n",
    "\n",
    "import _pickle as pickle\n",
    "import copy\n",
    "import jax\n",
    "import jax.random as jrandom\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision.datasets as torch_datasets\n",
    "\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from orbax.checkpoint import PyTreeCheckpointer, CheckpointManager\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cpu\"\n",
    "device = \"gpu:0\"\n",
    "get_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_width_pt = 750.0\n",
    "\n",
    "base_path = \"/home/bryanpu1/projects/icl_nc/jaxl/\"\n",
    "data_path = os.path.join(base_path, \"data\")\n",
    "log_path = os.path.join(base_path, \"jaxl/logs\")\n",
    "project_name = \"icl-mnist\"\n",
    "ablation_name = \"include_query_class-random_label\"\n",
    "run_name = (\n",
    "    \"default-03-17-24_13_01_35-af1cc36f-8698-4a61-a16a-4d2c726a22b9\"\n",
    "    # \"variable_len-03-17-24_13_04_06-cebe8d80-8ddc-432a-b170-a1f1e626f26a\"\n",
    ")\n",
    "\n",
    "learner_path = os.path.join(\n",
    "    log_path,\n",
    "    project_name,\n",
    "    ablation_name,\n",
    "    run_name,\n",
    ")\n",
    "\n",
    "exp_name = \"-\".join(run_name.split(\"-\")[:-8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict, config = load_config(learner_path)\n",
    "fixed_length = config.learner_config.dataset_config.dataset_wrapper.type in [\"FixedLengthContextDataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Train Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(\n",
    "    config.learner_config.dataset_config,\n",
    "    config.learner_config.seeds.data_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, model = load_model(\n",
    "    train_dataset.input_dim, train_dataset.output_dim, learner_path, -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_tasks = 30\n",
    "test_data_seed = 1000\n",
    "\n",
    "context_len = config.model_config.num_contexts\n",
    "num_samples_per_task = train_dataset._dataset.sequence_length - context_len\n",
    "sequence_length = train_dataset._dataset.sequence_length\n",
    "num_tasks = 100\n",
    "num_workers = 4\n",
    "\n",
    "print(num_samples_per_task, num_tasks, sequence_length, context_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dataset example\n",
    "def plot_examples(dataset, num_examples = 2, fixed_length=True):\n",
    "    offset = lambda ii: ii\n",
    "    for example_i in range(num_examples):\n",
    "        ci, co, q, l = dataset[\n",
    "            offset(example_i)\n",
    "        ]\n",
    "\n",
    "        nrows = 2\n",
    "        ncols = 8\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows,\n",
    "            ncols + 1,\n",
    "            figsize=set_size(doc_width_pt, 0.95, (nrows, ncols), False),\n",
    "            layout=\"constrained\",\n",
    "        )\n",
    "\n",
    "        for idx, (img, label) in enumerate(zip(ci, co)):\n",
    "            axes[idx // ncols, idx % ncols].imshow(img)\n",
    "            axes[idx // ncols, idx % ncols].set_title(np.argmax(label))\n",
    "            axes[idx // ncols, idx % ncols].axis('off')\n",
    "        axes[0, -1].axis('off')\n",
    "        axes[1, -1].axis('off')\n",
    "        axes[1, -1].imshow(q[0])\n",
    "        axes[1, -1].set_title(np.argmax(l, axis=-1))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "# Get model predictions\n",
    "def get_preds_labels(data_loader, num_tasks, max_label=None):\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "\n",
    "    for batch_i, samples in enumerate(data_loader):\n",
    "        if batch_i >= num_tasks:\n",
    "            break\n",
    "\n",
    "        (context_inputs, context_outputs, queries, one_hot_labels) = samples\n",
    "\n",
    "        outputs, _, _ = model.forward(\n",
    "            params[CONST_MODEL_DICT][CONST_MODEL],\n",
    "            queries.numpy(),\n",
    "            {\n",
    "                CONST_CONTEXT_INPUT: context_inputs.numpy(),\n",
    "                CONST_CONTEXT_OUTPUT: context_outputs.numpy(),\n",
    "            },\n",
    "            eval=True,\n",
    "        )\n",
    "        # return train_outputs, train_updates, outputs, updates\n",
    "        if max_label is None:\n",
    "            preds = np.argmax(outputs, axis=-1)\n",
    "        elif max_label == CONST_AUTO:\n",
    "            preds = np.argmax(outputs[..., :data_loader.dataset._data[\"num_classes\"]], axis=-1)\n",
    "        else:\n",
    "            preds = np.argmax(outputs[..., :max_label], axis=-1)\n",
    "        labels = np.argmax(one_hot_labels, axis=-1)\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "        all_outputs.append(outputs)\n",
    "\n",
    "    all_outputs = np.concatenate(all_outputs)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return all_preds, all_labels, all_outputs\n",
    "\n",
    "# Check model accuracy\n",
    "def print_performance(\n",
    "    all_preds,\n",
    "    all_labels,\n",
    "    output_dim,\n",
    "    context_len,\n",
    "    fixed_length=True,\n",
    "):\n",
    "    result_str = \"\"\n",
    "    conf_mat = confusion_matrix(all_labels, all_preds, labels=np.arange(output_dim))\n",
    "    acc = np.trace(conf_mat) / np.sum(conf_mat) * 100\n",
    "    result_str += \"Accuracy: {}%\\n\".format(acc)\n",
    "\n",
    "    if not fixed_length:\n",
    "        all_labels = all_labels.reshape((-1, context_len))\n",
    "        all_preds = all_preds.reshape((-1, context_len))\n",
    "\n",
    "        for len_i in range(context_len):\n",
    "            conf_mat = confusion_matrix(all_labels[:, len_i], all_preds[:, len_i], labels=np.arange(output_dim))\n",
    "            acc = np.trace(conf_mat) / np.sum(conf_mat) * 100\n",
    "            result_str += \"Context Length: {} - Accuracy: {}%\\n\".format(len_i + 1, acc)\n",
    "\n",
    "    return result_str\n",
    "\n",
    "# Complete evaluation\n",
    "def evaluate(exp_name, eval_name, dataset_config, seed, num_tasks, max_label, batch_size, context_len, num_workers, visualize=False, save=False, vis_num_samples=2, fixed_length=True):\n",
    "    dataset = get_dataset(\n",
    "        dataset_config,\n",
    "        seed,\n",
    "    )\n",
    "\n",
    "    if visualize:\n",
    "        plot_examples(dataset, vis_num_samples, fixed_length)\n",
    "\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=SequentialSampler(dataset),\n",
    "        drop_last=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    preds, labels, outputs = get_preds_labels(data_loader, num_tasks, max_label)\n",
    "    acc_str = print_performance(\n",
    "        preds,\n",
    "        labels,\n",
    "        dataset.output_dim[0],\n",
    "        context_len,\n",
    "        fixed_length,\n",
    "    )\n",
    "\n",
    "    if save:\n",
    "        save_dir = \"./evaluation-{}\".format(exp_name)\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        pickle.dump(\n",
    "            {\n",
    "                \"config\": dataset_config,\n",
    "                \"seed\": seed,\n",
    "                \"max_label\": max_label,\n",
    "                \"num_tasks\": num_tasks,\n",
    "                \"results\": {\n",
    "                    \"preds\": preds,\n",
    "                    \"labels\": labels,\n",
    "                    \"outputs\": outputs,\n",
    "                    \"acc_str\": acc_str,\n",
    "                }\n",
    "            },\n",
    "            open(os.path.join(save_dir, \"{}.pkl\".format(eval_name)), \"wb\"),\n",
    "        )\n",
    "    return acc_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining\n",
    "Pretraining is the exact same dataset for training the ICL model---we expect the performance to be near perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_acc = evaluate(\n",
    "    exp_name=exp_name,\n",
    "    eval_name=\"pretraining\",\n",
    "    dataset_config=config.learner_config.dataset_config,\n",
    "    seed=config.learner_config.seeds.data_seed,\n",
    "    num_tasks=num_tasks,\n",
    "    max_label=None,\n",
    "    batch_size=num_samples_per_task,\n",
    "    context_len=context_len,\n",
    "    num_workers=num_workers,\n",
    "    save=True,\n",
    "    fixed_length=fixed_length,\n",
    ")\n",
    "print(pretrain_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-distribution Evaluation\n",
    "This uses the pretraining image classes.  \n",
    "We consider the following evaluations:\n",
    "1. **Same pretraining data distribution accuracy**: This uses the same dataset as the pretraining but with a different seed.  \n",
    "This may change, for example, the augmentation or the sequences.\n",
    "\n",
    "1. **In-weight accuracy**: This uses totally random contexts and the model should predict purely using the query.  \n",
    "This follows from Chan et al. (2022).  \n",
    "Note: When $P(\\text{bursty}) = 0$ for pretraining, then this is the same as same pretraining data distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Same Pretraining Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_pretraining_config_dict = copy.deepcopy(\n",
    "    config_dict[\"learner_config\"][\"dataset_config\"]\n",
    ")\n",
    "same_pretraining_config_dict[\"dataset_kwargs\"][\"task_config\"][\"num_sequences\"] = num_test_tasks\n",
    "same_pretraining_config = parse_dict(same_pretraining_config_dict)\n",
    "\n",
    "same_pretraining_acc = evaluate(\n",
    "    exp_name=exp_name,\n",
    "    eval_name=\"same_pretraining\",\n",
    "    dataset_config=same_pretraining_config,\n",
    "    seed=test_data_seed,\n",
    "    num_tasks=num_test_tasks,\n",
    "    max_label=None,\n",
    "    batch_size=num_samples_per_task,\n",
    "    context_len=context_len,\n",
    "    num_workers=num_workers,\n",
    "    save=True,\n",
    "    fixed_length=fixed_length,\n",
    ")\n",
    "print(same_pretraining_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_weight_config_dict = copy.deepcopy(\n",
    "    config_dict[\"learner_config\"][\"dataset_config\"]\n",
    ")\n",
    "in_weight_config_dict[\"dataset_kwargs\"][\"task_config\"][\"num_sequences\"] = num_test_tasks\n",
    "in_weight_config_dict[\"dataset_kwargs\"][\"task_config\"][\"p_bursty\"] = 1.0\n",
    "in_weight_config = parse_dict(in_weight_config_dict)\n",
    "\n",
    "in_weight_acc = evaluate(\n",
    "    exp_name=exp_name,\n",
    "    eval_name=\"in_weight\",\n",
    "    dataset_config=in_weight_config,\n",
    "    seed=test_data_seed,\n",
    "    num_tasks=num_test_tasks,\n",
    "    max_label=None,\n",
    "    batch_size=num_samples_per_task,\n",
    "    context_len=context_len,\n",
    "    num_workers=num_workers,\n",
    "    save=True,\n",
    "    fixed_length=fixed_length,\n",
    ")\n",
    "print(in_weight_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-context Evaluation\n",
    "This uses heldout image classes.  \n",
    "We consider the following evaluations:\n",
    "1. **Complete out-of-distribution accuracy**: We use simply the heldout classes.  \n",
    "We constrain the model output to only be the valid classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Out-of-distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_config_dict = copy.deepcopy(\n",
    "    config_dict[\"learner_config\"][\"dataset_config\"]\n",
    ")\n",
    "ood_config_dict[\"dataset_kwargs\"][\"train\"] = False\n",
    "ood_config_dict[\"dataset_kwargs\"][\"task_config\"][\"num_sequences\"] = num_test_tasks\n",
    "ood_config = parse_dict(ood_config_dict)\n",
    "\n",
    "ood_acc = evaluate(\n",
    "    exp_name=exp_name,\n",
    "    eval_name=\"ood\",\n",
    "    dataset_config=ood_config,\n",
    "    seed=test_data_seed,\n",
    "    num_tasks=num_test_tasks,\n",
    "    max_label=CONST_AUTO,\n",
    "    batch_size=num_samples_per_task,\n",
    "    context_len=context_len,\n",
    "    num_workers=num_workers,\n",
    "    save=True,\n",
    "    visualize=False,\n",
    "    vis_num_samples=3,\n",
    "    fixed_length=fixed_length,\n",
    ")\n",
    "print(ood_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable-length Context Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_length_context_config_dict = copy.deepcopy(\n",
    "    config_dict[\"learner_config\"][\"dataset_config\"]\n",
    ")\n",
    "variable_length_context_config_dict[\"dataset_kwargs\"][\"train\"] = False\n",
    "variable_length_context_config_dict[\"dataset_kwargs\"][\"task_config\"][\"num_sequences\"] = num_test_tasks ** 2\n",
    "variable_length_context_config_dict[\"dataset_kwargs\"][\"task_config\"][\"augmentation\"] = False\n",
    "\n",
    "variable_length_context_config_dict[\"dataset_wrapper\"][\"type\"] = \"ContextDataset\"\n",
    "variable_length_context_config_dict[\"dataset_wrapper\"][\"kwargs\"][\"include_query_class\"] = True\n",
    "variable_length_context_config = parse_dict(variable_length_context_config_dict)\n",
    "\n",
    "variable_length_context_acc = evaluate(\n",
    "    exp_name=exp_name,\n",
    "    eval_name=\"variable_length_context\",\n",
    "    dataset_config=variable_length_context_config,\n",
    "    seed=test_data_seed,\n",
    "    num_tasks=num_test_tasks,\n",
    "    max_label=CONST_AUTO,\n",
    "    batch_size=context_len,\n",
    "    context_len=context_len,\n",
    "    num_workers=0,\n",
    "    save=True,\n",
    "    visualize=False,\n",
    "    vis_num_samples=24,\n",
    "    fixed_length=False,\n",
    ")\n",
    "print(variable_length_context_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accs = OrderedDict(\n",
    "    pretraining=pretrain_acc,\n",
    "    same_pretraining=same_pretraining_acc,\n",
    "    in_weight=in_weight_acc,\n",
    "    ood=ood_acc,\n",
    "    variable_length_context=variable_length_context_acc,\n",
    ")\n",
    "\n",
    "all_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
