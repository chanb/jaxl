{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaxl.constants import *\n",
    "from jaxl.datasets import get_dataset\n",
    "from jaxl.learning_utils import get_learner\n",
    "from jaxl.models.svm import *\n",
    "from jaxl.plot_utils import set_size\n",
    "from jaxl.utils import parse_dict\n",
    "\n",
    "import copy\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import os\n",
    "\n",
    "from functools import partial\n",
    "from orbax.checkpoint import PyTreeCheckpointer, CheckpointManager\n",
    "\n",
    "# plt.style.use(\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# base_dir = \"/Users/chanb/research/personal/jaxl/jaxl\"\n",
    "base_dir = \"/home/bryanpu1/projects/jaxl/jaxl\"\n",
    "\n",
    "# larger arch on Mac\n",
    "# rel_path = \"logs/icl-sigmoid_bce-context_len_8-large_arch/default-10-04-23_17_34_52-aa4fa4c8-db6a-4dd4-b91d-64d72b8b56a8\"\n",
    "# rel_path = \"logs/icl-sigmoid_bce-context_len_8-large_arch/all_ones-10-04-23_17_34_55-dbeed583-fe53-417e-9c7d-4f712dae126e\"\n",
    "# rel_path = \"logs/icl-sigmoid_bce-context_len_8-large_arch/one_hot-10-04-23_17_35_06-e36233bc-9649-4434-afa3-1e218a145e71/\"\n",
    "\n",
    "# num blocks = 8 on Salient 2\n",
    "rel_path = \"logs/icl-sigmoid_bce-context_len_16-num_blocks_8/one_hot-10-07-23_10_46_11-22362b3d-0b55-4c9f-af72-982e364d61a9\"\n",
    "\n",
    "learner_path = os.path.join(base_dir, rel_path)\n",
    "test_dataset_seed = 999\n",
    "sequence_len = 40\n",
    "num_tasks = 30\n",
    "# num_tasks = 1\n",
    "\n",
    "# For plotting\n",
    "doc_width_pt = 1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = os.path.join(learner_path, \"config.json\")\n",
    "with open(config_path, \"r\") as f:\n",
    "    config_dict = json.load(f)\n",
    "    config = parse_dict(config_dict)\n",
    "\n",
    "query_pred_only = getattr(config.model_config, \"query_pred_only\", False)\n",
    "print(config.learner_config.losses)\n",
    "use_sigmoid = config.learner_config.losses[0] == CONST_SIGMOID_BCE\n",
    "if query_pred_only:\n",
    "    if use_sigmoid:\n",
    "\n",
    "        def process_prediction(preds):\n",
    "            probs = jax.nn.sigmoid(preds.flatten())\n",
    "            return np.eye(2)[(probs >= 0.5).astype(int)]\n",
    "\n",
    "    else:\n",
    "\n",
    "        def process_prediction(preds):\n",
    "            return preds[:, -1]\n",
    "\n",
    "else:\n",
    "\n",
    "    def process_prediction(preds):\n",
    "        return preds[:, 0, -1]\n",
    "\n",
    "\n",
    "learner = get_learner(\n",
    "    config.learner_config, config.model_config, config.optimizer_config\n",
    ")\n",
    "\n",
    "checkpoint_manager = CheckpointManager(\n",
    "    os.path.join(learner_path, \"models\"),\n",
    "    PyTreeCheckpointer(),\n",
    ")\n",
    "\n",
    "params = checkpoint_manager.restore(checkpoint_manager.latest_step())\n",
    "params[CONST_MODEL_DICT][CONST_MODEL][CONST_POSITIONAL_ENCODING] = dict()\n",
    "llm_model = learner._model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[CONST_MODEL_DICT][CONST_MODEL][CONST_PREDICTOR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.learner_config.dataset_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_range = [-1.0, 1.0]\n",
    "ns_test_config = copy.deepcopy(vars(config.learner_config.dataset_config))\n",
    "\n",
    "ns_test_config[\"dataset_wrapper\"] = vars(ns_test_config[\"dataset_wrapper\"])\n",
    "ns_test_config[\"dataset_wrapper\"][\"type\"] = \"FixedLengthContextDataset\"\n",
    "ns_test_config[\"dataset_wrapper\"] = parse_dict(ns_test_config[\"dataset_wrapper\"])\n",
    "\n",
    "ns_test_config[\"dataset_kwargs\"] = vars(ns_test_config[\"dataset_kwargs\"])\n",
    "ns_test_config[\"dataset_kwargs\"][\"num_sequences\"] = num_tasks\n",
    "ns_test_config[\"dataset_kwargs\"][\"sequence_length\"] = sequence_len\n",
    "ns_test_config[\"dataset_kwargs\"][\"params_bound\"] = [-0.5, 0.5]\n",
    "ns_test_config[\"dataset_kwargs\"][\"inputs_range\"] = input_range\n",
    "# ns_test_config[\"dataset_kwargs\"][\"margin\"] = 0.5\n",
    "ns_test_config[\"dataset_kwargs\"] = parse_dict(ns_test_config[\"dataset_kwargs\"])\n",
    "ns_test_config = parse_dict(ns_test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = get_dataset(ns_test_config, seed=test_dataset_seed)\n",
    "unwrapped_dataset = test_dataset._dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_len = config.learner_config.dataset_config.dataset_wrapper.kwargs.context_len\n",
    "\n",
    "\n",
    "def get_result(dataset, task_i, context_len):\n",
    "    context_inputs, context_outputs = [], []\n",
    "    for context_i in range(context_len):\n",
    "        context_inputs.append(dataset._inputs[task_i, context_i])\n",
    "        context_outputs.append(dataset._targets[task_i, context_i])\n",
    "    context_inputs = np.stack(context_inputs)\n",
    "    context_outputs = np.stack(context_outputs)\n",
    "\n",
    "    queries = dataset._inputs[task_i, context_len:]\n",
    "    outputs = dataset._targets[task_i, context_len:]\n",
    "\n",
    "    preds, _ = jax.vmap(llm_model.forward, in_axes=[None, 0, None])(\n",
    "        params[CONST_MODEL_DICT][CONST_MODEL],\n",
    "        queries[:, None, None],\n",
    "        {\n",
    "            CONST_CONTEXT_INPUT: context_inputs[None, :],\n",
    "            CONST_CONTEXT_OUTPUT: context_outputs[None, :],\n",
    "        },\n",
    "    )\n",
    "    return queries, preds, outputs, context_inputs, context_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "\n",
    "for task_i in range(num_tasks):\n",
    "    queries, preds, outputs, context_inputs, context_outputs = get_result(\n",
    "        unwrapped_dataset, task_i, context_len\n",
    "    )\n",
    "    preds = process_prediction(preds)\n",
    "    res.setdefault(task_i, {})\n",
    "    res[task_i][\"data\"] = {\n",
    "        \"context_inputs\": context_inputs,\n",
    "        \"context_outputs\": context_outputs,\n",
    "        \"queries\": queries,\n",
    "        \"outputs\": outputs,\n",
    "    }\n",
    "    res[task_i][\"llm\"] = preds\n",
    "\n",
    "    gt = test_dataset.params[task_i]\n",
    "    res[task_i][\"gt\"] = -np.array(input_range) * gt[1] / gt[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_regs = [1e-2, 1e-1, 5e-1, 1.0, 2.0, 10.0, 100.0, 1000.0]\n",
    "lr_regs = [1e-2, 1e-1, 5e-1, 1.0, 2.0, 10.0, 100.0, 1000.0]\n",
    "knn_ks = [1, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_knn(inputs, outputs, num_neighbours):\n",
    "    knn = make_pipeline(\n",
    "        KNeighborsClassifier(\n",
    "            n_neighbors=num_neighbours,\n",
    "        )\n",
    "    )\n",
    "    knn.fit(inputs, np.argmax(outputs, axis=1))\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_i in range(num_tasks):\n",
    "    knns = {}\n",
    "    context_inputs = res[task_i][\"data\"][\"context_inputs\"]\n",
    "    context_outputs = res[task_i][\"data\"][\"context_outputs\"]\n",
    "\n",
    "    for idx, k in enumerate(knn_ks):\n",
    "        knns[k] = make_knn(context_inputs, context_outputs, k)\n",
    "\n",
    "    res[task_i][\"knn\"] = knns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_svm(inputs, outputs, reg_coef):\n",
    "    svm = make_pipeline(\n",
    "        LinearSVC(\n",
    "            C=reg_coef,\n",
    "            max_iter=2000,\n",
    "        ),\n",
    "    )\n",
    "    svm.fit(inputs, np.argmax(outputs, axis=1))\n",
    "    return svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_info = {}\n",
    "\n",
    "for task_i in range(num_tasks):\n",
    "    svms = {}\n",
    "    svm_info.setdefault(task_i, {})\n",
    "\n",
    "    context_inputs = res[task_i][\"data\"][\"context_inputs\"]\n",
    "    context_outputs = res[task_i][\"data\"][\"context_outputs\"]\n",
    "\n",
    "    for idx, svm_reg in enumerate(svm_regs):\n",
    "        svms[svm_reg] = make_svm(context_inputs, context_outputs, svm_reg)\n",
    "\n",
    "        decision_function = svms[svm_reg].decision_function(context_inputs)\n",
    "        support_vector_indices = np.where(np.abs(decision_function) <= 1 + 1e-15)[0]\n",
    "        support_vectors = context_inputs[support_vector_indices]\n",
    "\n",
    "        svm_info[task_i][svm_reg] = {\n",
    "            \"support_vectors\": support_vectors,\n",
    "            \"support_labels\": context_outputs[support_vector_indices],\n",
    "            \"support_vector_indices\": support_vector_indices,\n",
    "        }\n",
    "\n",
    "    res[task_i][\"svm\"] = svms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lr(inputs, outputs, penalty, reg_coef):\n",
    "    logistic_regression = make_pipeline(\n",
    "        LogisticRegression(\n",
    "            penalty=penalty,\n",
    "            C=reg_coef,\n",
    "            max_iter=2000,\n",
    "        )\n",
    "    )\n",
    "    logistic_regression.fit(inputs, np.argmax(outputs, axis=1))\n",
    "    return logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_i in range(num_tasks):\n",
    "    lrs = {}\n",
    "\n",
    "    context_inputs = res[task_i][\"data\"][\"context_inputs\"]\n",
    "    context_outputs = res[task_i][\"data\"][\"context_outputs\"]\n",
    "\n",
    "    for idx, lr_reg in enumerate(lr_regs):\n",
    "        lrs[lr_reg] = make_lr(context_inputs, context_outputs, \"l2\", lr_reg)\n",
    "    res[task_i][\"lr\"] = lrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICL Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_preds = {}\n",
    "num_models = len(lr_regs) + len(svm_regs)\n",
    "\n",
    "delta = 0.01\n",
    "xs_grid = np.arange(-1.0, 1.0 + delta, delta)\n",
    "test_queries = np.stack(np.meshgrid(xs_grid, xs_grid)).reshape((2, -1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_i in range(num_tasks):\n",
    "    context_inputs = res[task_i][\"data\"][\"context_inputs\"]\n",
    "    context_outputs = res[task_i][\"data\"][\"context_outputs\"]\n",
    "\n",
    "    model_preds.setdefault(task_i, {})\n",
    "    llm_preds, _ = jax.vmap(llm_model.forward, in_axes=[None, 0, None])(\n",
    "        params[CONST_MODEL_DICT][CONST_MODEL],\n",
    "        test_queries[:, None, None],\n",
    "        {\n",
    "            CONST_CONTEXT_INPUT: context_inputs[None, :],\n",
    "            CONST_CONTEXT_OUTPUT: context_outputs[None, :],\n",
    "        },\n",
    "    )\n",
    "    llm_preds = process_prediction(llm_preds)\n",
    "    model_preds[task_i][\"llm\"] = llm_preds\n",
    "    model_preds[task_i][\"gt\"] = np.eye(2)[\n",
    "        (\n",
    "            (\n",
    "                test_queries @ test_dataset.params[task_i, 1:]\n",
    "                + test_dataset.params[task_i, :1]\n",
    "            )\n",
    "            >= 0\n",
    "        )\n",
    "        .flatten()\n",
    "        .astype(int)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task_i in range(num_tasks):\n",
    "    ncols = 4\n",
    "    nrows = math.ceil(num_models / ncols)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows,\n",
    "        ncols,\n",
    "        figsize=set_size(doc_width_pt, 0.95, (nrows, ncols), False),\n",
    "        layout=\"constrained\",\n",
    "    )\n",
    "\n",
    "    model_classes = {\n",
    "        \"SVM\": res[task_i][\"svm\"],\n",
    "        \"LR\": res[task_i][\"lr\"],\n",
    "    }\n",
    "\n",
    "    llm_preds = model_preds[task_i][\"llm\"]\n",
    "    llm_pred_labels = np.argmax(llm_preds, axis=-1)\n",
    "\n",
    "    model_i = 0\n",
    "    for model_class, models in model_classes.items():\n",
    "        for reg_coef, model in models.items():\n",
    "            model_out = (\n",
    "                -(np.array(input_range) * model[0].coef_[0, 0] + model[0].intercept_[0])\n",
    "                / model[0].coef_[0, 1]\n",
    "            )\n",
    "\n",
    "            if nrows == 1:\n",
    "                ax = axes[model_i]\n",
    "            else:\n",
    "                ax = axes[model_i // ncols, model_i % ncols]\n",
    "\n",
    "            for possible_label in [0, 1]:\n",
    "                idxes = np.where(llm_pred_labels == possible_label)\n",
    "                ax.scatter(\n",
    "                    test_queries[idxes][:, 0],\n",
    "                    test_queries[idxes][:, 1],\n",
    "                    label=f\"{possible_label}\" if model_i == 0 else \"\",\n",
    "                    s=5,\n",
    "                )\n",
    "\n",
    "            if model_class == \"SVM\":\n",
    "                ax.scatter(\n",
    "                    svm_info[task_i][reg_coef][\"support_vectors\"][:, 0],\n",
    "                    svm_info[task_i][reg_coef][\"support_vectors\"][:, 1],\n",
    "                    label=\"support vector\" if model_i == 0 else \"\",\n",
    "                    color=\"black\",\n",
    "                )\n",
    "\n",
    "            ax.plot(\n",
    "                np.array(input_range),\n",
    "                res[task_i][\"gt\"],\n",
    "                label=\"Ground truth\" if model_i == 0 else \"\",\n",
    "                color=\"black\",\n",
    "                linewidth=1,\n",
    "            )\n",
    "\n",
    "            ax.plot(\n",
    "                np.array(input_range),\n",
    "                model_out,\n",
    "                color=\"blue\",\n",
    "                linewidth=1,\n",
    "                label=\"Comparator\" if model_i == 0 else \"\",\n",
    "            )\n",
    "\n",
    "            ax.set_xlim(input_range[0] - 0.01, input_range[1] + 0.01)\n",
    "            ax.set_ylim(input_range[0] - 0.01, input_range[1] + 0.01)\n",
    "            ax.set_title(f\"{model_class} {reg_coef}\")\n",
    "            model_i += 1\n",
    "\n",
    "    fig.legend(\n",
    "        bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "        loc=\"lower center\",\n",
    "        ncols=4,\n",
    "        borderaxespad=0.0,\n",
    "        frameon=True,\n",
    "        fontsize=\"8\",\n",
    "    )\n",
    "    fig.supxlabel(\"$x_1$\")\n",
    "    fig.supylabel(\"$x_2$\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean 0-1 Prediction Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {}\n",
    "\n",
    "for task_i in range(num_tasks):\n",
    "    model_classes = {\n",
    "        \"SVM\": res[task_i][\"svm\"],\n",
    "        \"LR\": res[task_i][\"lr\"],\n",
    "        \"KNN\": res[task_i][\"knn\"],\n",
    "    }\n",
    "\n",
    "    llm_preds = np.argmax(model_preds[task_i][\"llm\"], axis=1)\n",
    "    for model_class, models in model_classes.items():\n",
    "        for reg_coef, model in models.items():\n",
    "            losses.setdefault((model_class, reg_coef), [])\n",
    "            losses[(model_class, reg_coef)].append(\n",
    "                np.mean(model.predict(test_queries) == llm_preds)\n",
    "            )\n",
    "\n",
    "for model_info, curr_loss in losses.items():\n",
    "    print(\n",
    "        \"{}: {:.2f}% +/- {:.2f}\".format(\n",
    "            model_info, np.mean(curr_loss) * 100, np.std(curr_loss * 100)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Using Support Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if query_pred_only:\n",
    "\n",
    "    def get_new_pairs(support_vectors, support_labels, query, context_len):\n",
    "        support_vectors = jnp.concatenate(\n",
    "            (\n",
    "                jnp.zeros(\n",
    "                    (context_len - len(support_vectors), *support_vectors.shape[1:])\n",
    "                ),\n",
    "                support_vectors,\n",
    "                query,\n",
    "            )\n",
    "        )\n",
    "        support_labels = jnp.concatenate(\n",
    "            (\n",
    "                jnp.zeros(\n",
    "                    (context_len - len(support_labels), *support_labels.shape[1:])\n",
    "                ),\n",
    "                support_labels,\n",
    "            )\n",
    "        )\n",
    "        return support_vectors, support_labels\n",
    "\n",
    "else:\n",
    "\n",
    "    def get_new_pairs(support_vectors, support_labels, query, context_len):\n",
    "        support_vectors = jnp.concatenate(\n",
    "            (\n",
    "                support_vectors,\n",
    "                query,\n",
    "                jnp.zeros(\n",
    "                    (context_len - len(support_vectors), *support_vectors.shape[1:])\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        support_labels = jnp.concatenate(\n",
    "            (\n",
    "                support_labels,\n",
    "                jnp.zeros(\n",
    "                    (context_len - len(support_labels), *support_labels.shape[1:])\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "        return support_vectors, support_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_res = {}\n",
    "\n",
    "for task_i in range(num_tasks):\n",
    "    context_res.setdefault(task_i, {})\n",
    "\n",
    "    ncols = 4\n",
    "    nrows = math.ceil(len(svm_regs) / ncols)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows + 2,\n",
    "        ncols,\n",
    "        figsize=set_size(doc_width_pt, 0.95, (nrows + 2, ncols), False),\n",
    "        layout=\"constrained\",\n",
    "    )\n",
    "\n",
    "    context_inputs = res[task_i][\"data\"][\"context_inputs\"]\n",
    "    context_outputs = res[task_i][\"data\"][\"context_outputs\"]\n",
    "\n",
    "    for idx, svm_reg in enumerate(svm_regs):\n",
    "        if nrows == 1:\n",
    "            ax = axes[idx]\n",
    "        else:\n",
    "            ax = axes[idx // ncols, idx % ncols]\n",
    "        support_vectors = svm_info[task_i][svm_reg][\"support_vectors\"]\n",
    "        support_labels = svm_info[task_i][svm_reg][\"support_labels\"]\n",
    "\n",
    "        num_support_vectors = len(support_vectors)\n",
    "        inputs, outputs = jax.vmap(get_new_pairs, in_axes=[None, None, 0, None])(\n",
    "            support_vectors, support_labels, test_queries[:, None], context_len\n",
    "        )\n",
    "\n",
    "        # Use support vectors as context\n",
    "        llm_preds, _ = llm_model.forward(\n",
    "            params[CONST_MODEL_DICT][CONST_MODEL],\n",
    "            inputs[:, [-1]],\n",
    "            {\n",
    "                CONST_CONTEXT_INPUT: inputs[:, :-1],\n",
    "                CONST_CONTEXT_OUTPUT: outputs,\n",
    "            },\n",
    "        )\n",
    "        if not query_pred_only:\n",
    "            llm_preds = llm_preds[:, num_support_vectors]\n",
    "        elif use_sigmoid:\n",
    "            llm_preds = process_prediction(llm_preds)\n",
    "        context_res[task_i][\"llm\"] = llm_preds\n",
    "        context_res[task_i][\"support_vectors\"] = support_vectors\n",
    "        context_res[task_i][\"support_labels\"] = support_labels\n",
    "\n",
    "        llm_pred_labels = np.argmax(llm_preds, axis=-1)\n",
    "\n",
    "        for possible_label in [0, 1]:\n",
    "            idxes = np.where(llm_pred_labels == possible_label)\n",
    "            if len(idxes[0]) == 0:\n",
    "                continue\n",
    "            ax.scatter(\n",
    "                test_queries[idxes][:, 0],\n",
    "                test_queries[idxes][:, 1],\n",
    "                label=f\"{possible_label}\" if model_i == 0 else \"\",\n",
    "                s=5,\n",
    "            )\n",
    "\n",
    "        ax.plot(\n",
    "            np.array(input_range),\n",
    "            res[task_i][\"gt\"],\n",
    "            label=\"Ground truth\" if idx == 0 else \"\",\n",
    "            color=\"red\",\n",
    "            alpha=0.3,\n",
    "        )\n",
    "\n",
    "        ax.scatter(\n",
    "            context_inputs[:, 0],\n",
    "            context_inputs[:, 1],\n",
    "            c=context_outputs[:, -1],\n",
    "            s=30,\n",
    "            cmap=plt.cm.Paired,\n",
    "        )\n",
    "        DecisionBoundaryDisplay.from_estimator(\n",
    "            res[task_i][\"svm\"][svm_reg],\n",
    "            context_inputs,\n",
    "            ax=ax,\n",
    "            grid_resolution=50,\n",
    "            plot_method=\"contour\",\n",
    "            colors=\"k\",\n",
    "            levels=[-1, 0, 1],\n",
    "            alpha=0.5,\n",
    "            linestyles=[\"--\", \"-\", \"--\"],\n",
    "        )\n",
    "        ax.scatter(\n",
    "            svm_info[task_i][svm_reg][\"support_vectors\"][:, 0],\n",
    "            svm_info[task_i][svm_reg][\"support_vectors\"][:, 1],\n",
    "            s=100,\n",
    "            linewidth=1,\n",
    "            facecolors=\"none\",\n",
    "            edgecolors=\"k\",\n",
    "        )\n",
    "        ax.set_xlim(input_range[0] - 0.01, input_range[1] + 0.01)\n",
    "        ax.set_ylim(input_range[0] - 0.01, input_range[1] + 0.01)\n",
    "        ax.set_title(f\"Reg. Coef.: {svm_reg}\")\n",
    "\n",
    "    for idx, k in enumerate([1, 3, 5, 7]):\n",
    "        if nrows == 1:\n",
    "            ax = axes[idx]\n",
    "        else:\n",
    "            ax = axes[nrows, idx % ncols]\n",
    "\n",
    "        knn = make_knn(context_inputs, context_outputs, k)\n",
    "        knn_preds = knn.predict(test_queries)\n",
    "\n",
    "        for possible_label in [0, 1]:\n",
    "            idxes = np.where(knn_preds == possible_label)\n",
    "            if len(idxes[0]) == 0:\n",
    "                continue\n",
    "            ax.scatter(\n",
    "                test_queries[idxes][:, 0],\n",
    "                test_queries[idxes][:, 1],\n",
    "                label=f\"{possible_label}\" if model_i == 0 else \"\",\n",
    "                s=5,\n",
    "            )\n",
    "        ax.set_xlim(input_range[0] - 0.01, input_range[1] + 0.01)\n",
    "        ax.set_ylim(input_range[0] - 0.01, input_range[1] + 0.01)\n",
    "        ax.set_title(f\"K: {k}\")\n",
    "\n",
    "    for idx, k in enumerate([1, 3, 5, 7]):\n",
    "        if nrows == 1:\n",
    "            ax = axes[idx]\n",
    "        else:\n",
    "            ax = axes[nrows + 1, idx % ncols]\n",
    "\n",
    "        ax.set_title(f\"K: {k}\")\n",
    "        ax.set_xlim(input_range[0] - 0.01, input_range[1] + 0.01)\n",
    "        ax.set_ylim(input_range[0] - 0.01, input_range[1] + 0.01)\n",
    "        if len(support_vectors) < k:\n",
    "            continue\n",
    "\n",
    "        knn = make_knn(support_vectors, support_labels, k)\n",
    "        knn_preds = knn.predict(test_queries)\n",
    "        for possible_label in [0, 1]:\n",
    "            idxes = np.where(knn_preds == possible_label)\n",
    "            if len(idxes[0]) == 0:\n",
    "                continue\n",
    "            ax.scatter(\n",
    "                test_queries[idxes][:, 0],\n",
    "                test_queries[idxes][:, 1],\n",
    "                label=f\"{possible_label}\" if model_i == 0 else \"\",\n",
    "                s=5,\n",
    "            )\n",
    "\n",
    "    fig.legend(\n",
    "        bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "        loc=\"lower center\",\n",
    "        ncols=3,\n",
    "        borderaxespad=0.0,\n",
    "        frameon=True,\n",
    "        fontsize=\"8\",\n",
    "    )\n",
    "    fig.supxlabel(\"$x_1$\")\n",
    "    fig.supylabel(\"$x_2$\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_losses = {}\n",
    "\n",
    "for task_i in range(num_tasks):\n",
    "    model_classes = {\n",
    "        \"SVM\": res[task_i][\"svm\"],\n",
    "        \"LR\": res[task_i][\"lr\"],\n",
    "        \"KNN\": res[task_i][\"knn\"],\n",
    "    }\n",
    "\n",
    "    llm_preds = np.argmax(context_res[task_i][\"llm\"], axis=1)\n",
    "    for model_class, models in model_classes.items():\n",
    "        for reg_coef, model in models.items():\n",
    "            support_vector_losses.setdefault((model_class, reg_coef), [])\n",
    "            support_vector_losses[(model_class, reg_coef)].append(\n",
    "                np.mean(model.predict(test_queries) == llm_preds)\n",
    "            )\n",
    "\n",
    "for model_info, curr_loss in support_vector_losses.items():\n",
    "    print(\n",
    "        \"{}: {:.2f}% +/- {:.2f}\".format(\n",
    "            model_info, np.mean(curr_loss) * 100, np.std(curr_loss * 100)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permute Support Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permute_res = {}\n",
    "\n",
    "num_permutations = 4\n",
    "perm_seed = 9999\n",
    "\n",
    "permutation_keys = jrandom.split(jrandom.PRNGKey(perm_seed), 4)\n",
    "\n",
    "for task_i in range(num_tasks):\n",
    "    permute_res.setdefault(task_i, {})\n",
    "\n",
    "    ncols = 4\n",
    "    nrows = math.ceil((len(svm_regs) * num_permutations) / ncols)\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows,\n",
    "        ncols,\n",
    "        figsize=set_size(doc_width_pt, 0.95, (nrows, ncols), False),\n",
    "        layout=\"constrained\",\n",
    "    )\n",
    "\n",
    "    context_inputs = res[task_i][\"data\"][\"context_inputs\"]\n",
    "    context_outputs = res[task_i][\"data\"][\"context_outputs\"]\n",
    "\n",
    "    ax_idx = 0\n",
    "    for idx, svm_reg in enumerate(svm_regs):\n",
    "        for permutation_key in permutation_keys:\n",
    "            if nrows == 1:\n",
    "                ax = axes[ax_idx]\n",
    "            else:\n",
    "                ax = axes[ax_idx // ncols, ax_idx % ncols]\n",
    "            support_vectors = svm_info[task_i][svm_reg][\"support_vectors\"]\n",
    "            support_labels = svm_info[task_i][svm_reg][\"support_labels\"]\n",
    "\n",
    "            perm_idxes = jrandom.permutation(\n",
    "                permutation_key, x=np.arange(len(support_vectors))\n",
    "            )\n",
    "            support_vectors = support_vectors[perm_idxes]\n",
    "            support_labels = support_labels[perm_idxes]\n",
    "\n",
    "            num_support_vectors = len(support_vectors)\n",
    "            inputs, outputs = jax.vmap(get_new_pairs, in_axes=[None, None, 0, None])(\n",
    "                support_vectors, support_labels, test_queries[:, None], context_len\n",
    "            )\n",
    "\n",
    "            # Use support vectors as context\n",
    "            llm_preds, _ = llm_model.forward(\n",
    "                params[CONST_MODEL_DICT][CONST_MODEL],\n",
    "                inputs[:, [-1]],\n",
    "                {\n",
    "                    CONST_CONTEXT_INPUT: inputs[:, :-1],\n",
    "                    CONST_CONTEXT_OUTPUT: outputs,\n",
    "                },\n",
    "            )\n",
    "            if not query_pred_only:\n",
    "                llm_preds = llm_preds[:, num_support_vectors]\n",
    "            elif use_sigmoid:\n",
    "                llm_preds = process_prediction(llm_preds)\n",
    "            permute_res[task_i][\"llm\"] = llm_preds\n",
    "            permute_res[task_i][\"support_vectors\"] = support_vectors\n",
    "            permute_res[task_i][\"support_labels\"] = support_labels\n",
    "\n",
    "            llm_pred_labels = np.argmax(llm_preds, axis=-1)\n",
    "\n",
    "            for possible_label in [0, 1]:\n",
    "                idxes = np.where(llm_pred_labels == possible_label)\n",
    "                if len(idxes[0]) == 0:\n",
    "                    continue\n",
    "                ax.scatter(\n",
    "                    test_queries[idxes][:, 0],\n",
    "                    test_queries[idxes][:, 1],\n",
    "                    label=f\"{possible_label}\" if ax_idx == 0 else \"\",\n",
    "                    s=5,\n",
    "                )\n",
    "\n",
    "            ax.plot(\n",
    "                np.array(input_range),\n",
    "                res[task_i][\"gt\"],\n",
    "                label=\"Ground truth\" if ax_idx == 0 else \"\",\n",
    "                color=\"red\",\n",
    "                alpha=0.3,\n",
    "            )\n",
    "\n",
    "            ax.scatter(\n",
    "                context_inputs[:, 0],\n",
    "                context_inputs[:, 1],\n",
    "                c=context_outputs[:, -1],\n",
    "                s=30,\n",
    "                cmap=plt.cm.Paired,\n",
    "            )\n",
    "            DecisionBoundaryDisplay.from_estimator(\n",
    "                res[task_i][\"svm\"][svm_reg],\n",
    "                context_inputs,\n",
    "                ax=ax,\n",
    "                grid_resolution=50,\n",
    "                plot_method=\"contour\",\n",
    "                colors=\"k\",\n",
    "                levels=[-1, 0, 1],\n",
    "                alpha=0.5,\n",
    "                linestyles=[\"--\", \"-\", \"--\"],\n",
    "            )\n",
    "            ax.scatter(\n",
    "                svm_info[task_i][svm_reg][\"support_vectors\"][:, 0],\n",
    "                svm_info[task_i][svm_reg][\"support_vectors\"][:, 1],\n",
    "                s=100,\n",
    "                linewidth=1,\n",
    "                facecolors=\"none\",\n",
    "                edgecolors=\"k\",\n",
    "            )\n",
    "            ax.set_xlim(input_range[0] - 0.01, input_range[1] + 0.01)\n",
    "            ax.set_ylim(input_range[0] - 0.01, input_range[1] + 0.01)\n",
    "            ax.set_title(f\"Reg. Coef.: {svm_reg}\")\n",
    "            ax_idx += 1\n",
    "\n",
    "    fig.legend(\n",
    "        bbox_to_anchor=(0.0, 1.0, 1.0, 0.0),\n",
    "        loc=\"lower center\",\n",
    "        ncols=3,\n",
    "        borderaxespad=0.0,\n",
    "        frameon=True,\n",
    "        fontsize=\"8\",\n",
    "    )\n",
    "    fig.supxlabel(\"$x_1$\")\n",
    "    fig.supylabel(\"$x_2$\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examplar Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_len_res = {\n",
    "    \"llm\": {},\n",
    "    \"lr\": {},\n",
    "    \"svm\": {},\n",
    "}\n",
    "loss_per_examplar_len = {}\n",
    "\n",
    "for task_i in range(num_tasks):\n",
    "    context_inputs = res[task_i][\"data\"][\"context_inputs\"]\n",
    "    context_outputs = res[task_i][\"data\"][\"context_outputs\"]\n",
    "\n",
    "    for examplar_len in range(2, context_len):\n",
    "        for model_class in ex_len_res:\n",
    "            ex_len_res[model_class].setdefault(examplar_len, [])\n",
    "        loss_per_examplar_len.setdefault(examplar_len, [])\n",
    "        inputs, outputs = jax.vmap(get_new_pairs, in_axes=[None, None, 0, None])(\n",
    "            context_inputs[:examplar_len],\n",
    "            context_outputs[:examplar_len],\n",
    "            test_queries[:, None],\n",
    "            context_len,\n",
    "        )\n",
    "\n",
    "        # Use support vectors as context\n",
    "        llm_preds, _ = llm_model.forward(\n",
    "            params[CONST_MODEL_DICT][CONST_MODEL],\n",
    "            inputs[:, [-1]],\n",
    "            {\n",
    "                CONST_CONTEXT_INPUT: inputs[:, :-1],\n",
    "                CONST_CONTEXT_OUTPUT: outputs,\n",
    "            },\n",
    "        )\n",
    "        if not query_pred_only:\n",
    "            llm_preds = llm_preds[:, examplar_len]\n",
    "        elif use_sigmoid:\n",
    "            llm_preds = process_prediction(llm_preds)\n",
    "        loss = np.mean(\n",
    "            np.argmax(llm_preds, axis=-1)\n",
    "            != np.argmax(model_preds[task_i][\"gt\"], axis=-1)\n",
    "        )\n",
    "        ex_len_res[\"llm\"][examplar_len].append(loss)\n",
    "\n",
    "        if len(np.unique(np.argmax(context_outputs[:examplar_len], axis=-1))) > 1:\n",
    "            svm = make_svm(\n",
    "                context_inputs[:examplar_len],\n",
    "                context_outputs[:examplar_len],\n",
    "                svm_regs[-1],\n",
    "            )\n",
    "            preds = svm.predict(test_queries)\n",
    "            loss = np.mean(preds != np.argmax(model_preds[task_i][\"gt\"], axis=-1))\n",
    "            ex_len_res[\"svm\"][examplar_len].append(loss)\n",
    "            lr = make_lr(\n",
    "                context_inputs[:examplar_len],\n",
    "                context_outputs[:examplar_len],\n",
    "                \"l2\",\n",
    "                lr_regs[-1],\n",
    "            )\n",
    "            preds = lr.predict(test_queries)\n",
    "            loss = np.mean(preds != np.argmax(model_preds[task_i][\"gt\"], axis=-1))\n",
    "            ex_len_res[\"lr\"][examplar_len].append(loss)\n",
    "\n",
    "        loss_per_examplar_len[examplar_len].append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for examplar_len, losses in loss_per_examplar_len.items():\n",
    "    print(\n",
    "        \"Examplar Length: {}, Loss - mean: {} std: {}\".format(\n",
    "            examplar_len, np.mean(losses), np.std(losses)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 1\n",
    "nrows = 1\n",
    "fig, ax = plt.subplots(\n",
    "    nrows,\n",
    "    ncols,\n",
    "    figsize=set_size(750, 0.95, (nrows, ncols), True),\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "for model_class, examplar_lens in ex_len_res.items():\n",
    "    xs = np.array((list(examplar_lens.keys())))\n",
    "    ys = np.array([np.mean(losses) for losses in examplar_lens.values()])\n",
    "    stds = np.array([np.std(losses) for losses in examplar_lens.values()])\n",
    "    sort_idxes = np.argsort(xs)\n",
    "    xs = xs[sort_idxes]\n",
    "    ys = ys[sort_idxes]\n",
    "    ax.plot(xs, ys, label=model_class)\n",
    "    ax.fill_between(xs, ys + stds, ys - stds, alpha=0.1)\n",
    "ax.legend()\n",
    "ax.set_ylabel(\"0-1 Loss\")\n",
    "ax.set_xlabel(\"Examplar Length\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check SVM in Representation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_i = 0\n",
    "context_inputs = res[task_i][\"data\"][\"context_inputs\"]\n",
    "context_outputs = res[task_i][\"data\"][\"context_outputs\"]\n",
    "\n",
    "print(test_queries.shape, context_inputs.shape)\n",
    "\n",
    "inputs, outputs = jax.vmap(get_new_pairs, in_axes=[None, None, 0, None])(\n",
    "    context_inputs, context_outputs, context_inputs[:, None], context_len\n",
    ")\n",
    "\n",
    "print(inputs.shape, outputs.shape)\n",
    "\n",
    "repr, _ = llm_model.get_latent(\n",
    "    params[CONST_MODEL_DICT][CONST_MODEL],\n",
    "    inputs[:, [-1]],\n",
    "    {\n",
    "        CONST_CONTEXT_INPUT: inputs[:, :-1],\n",
    "        CONST_CONTEXT_OUTPUT: outputs,\n",
    "    },\n",
    ")\n",
    "print(repr.shape)\n",
    "\n",
    "last_reprs = repr[:, -1]\n",
    "print(last_reprs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.argmax(context_outputs, axis=-1)\n",
    "train_y[train_y == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, primal_sol_repr = primal_svm(last_reprs, train_y)\n",
    "print(loss)\n",
    "\n",
    "loss, primal_sol_input = primal_svm(context_inputs, train_y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, dual_sol_repr = dual_svm(last_reprs, train_y)\n",
    "print(loss)\n",
    "print(np.where(np.abs(dual_sol_repr) > 1e-10))\n",
    "\n",
    "loss, dual_sol_input = dual_svm(context_inputs, train_y)\n",
    "print(loss)\n",
    "print(np.where(np.abs(dual_sol_input) > 1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly sanity check support vectors with Scikit-Learn vs CVXPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_svm = make_svm(last_reprs, context_outputs, 1000.0)\n",
    "decision_function = repr_svm.decision_function(last_reprs)\n",
    "support_vector_indices = np.where(np.abs(decision_function) <= 1)[0]\n",
    "print(support_vector_indices)\n",
    "print(svm_info[task_i][svm_regs[-1]][\"support_vector_indices\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*primal_sol_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_weights = np.sum((dual_sol_input * train_y)[:, None] * context_inputs, axis=0)\n",
    "\n",
    "support_vector_index = np.where(np.abs(dual_sol_input) > 1e-10)[0][0]\n",
    "recovered_bias = train_y[support_vector_index] - np.dot(\n",
    "    recovered_weights, context_inputs[support_vector_index]\n",
    ")\n",
    "print(*recovered_weights, recovered_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recover the primal parameters from the dual derived in representation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_weights = np.sum((dual_sol_repr * train_y)[:, None] * context_inputs, axis=0)\n",
    "\n",
    "support_vector_index = np.where(np.abs(dual_sol_repr) > 1e-10)[0][0]\n",
    "recovered_bias = train_y[support_vector_index] - np.dot(\n",
    "    recovered_weights, context_inputs[support_vector_index]\n",
    ")\n",
    "print(*recovered_weights, recovered_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the dual parameters from the input and representation spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dual_sol_input)\n",
    "print(dual_sol_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_sigmoid:\n",
    "    (\n",
    "        params[CONST_MODEL_DICT][CONST_MODEL][CONST_PREDICTOR][\"params\"][\"kernel\"][:, 0]\n",
    "        - params[CONST_MODEL_DICT][CONST_MODEL][CONST_PREDICTOR][\"params\"][\"kernel\"][\n",
    "            :, 1\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primal_sol_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance of primal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not use_sigmoid:\n",
    "    np.linalg.norm(\n",
    "        params[CONST_MODEL_DICT][CONST_MODEL][CONST_PREDICTOR][\"params\"][\"kernel\"][:, 0]\n",
    "        - params[CONST_MODEL_DICT][CONST_MODEL][CONST_PREDICTOR][\"params\"][\"kernel\"][\n",
    "            :, 1\n",
    "        ]\n",
    "        - primal_sol_repr[:-1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
