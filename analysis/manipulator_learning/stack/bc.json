{
    "logging_config": {
        "save_path": "./logs/manipulator_learning/stack",
        "experiment_name": "bc-10k_steps",
        "log_interval": 1,
        "checkpoint_interval": 1000
    },
    "model_config": {
        "architecture": "mlp",
        "layers": [
            256,
            256,
            256
        ],
        "activation": "tanh",
        "flatten": true
    },
    "optimizer_config": {
        "optimizer": "adam",
        "lr": {
            "scheduler": "constant_schedule",
            "scheduler_kwargs": {
                "value": 0.0003
            }
        },
        "max_grad_norm": false
    },
    "learner_config": {
        "task": "imitation_learning",
        "buffer_config": {
            "load_buffer": "/home/bryan/research/lfgp/lfgp_data/custom_expert_data/stack/1000000_steps_no_extra_final/int_0.gz",
            "buffer_type": "default",
            "set_size": 10000
        },
        "seeds": {
            "model_seed": 927,
            "buffer_seed": 927
        },
        "learner": "bc",
        "num_updates_per_epoch": 100,
        "batch_size": 256,
        "obs_rms": false,
        "policy_distribution": "deterministic",
        "losses": [
            "gaussian",
            "l2"
        ],
        "loss_settings": [
            {
                "coefficient": 1.0,
                "reduction": "mean"
            },
            {
                "coefficient": 0.0
            }
        ]
    },
    "train_config": {
        "num_epochs": 5000
    }
}