{
    "logging_config": {
        "save_path": "./logs/manipulator_learning/stack",
        "experiment_name": "residual-rlpd-cross_q-deterministic_exploration-wide_critic-pretrain",
        "log_interval": 1,
        "checkpoint_interval": 10
    },
    "model_config": {
        "backbone": {
            "architecture": "mlp",
            "layers": [256, 256, 256],
            "activation": "tanh",
            "flatten": true,
            "policy_distribution": "deterministic",
            "pretrained_model": "/home/bryan/research/jaxl/logs/manipulator_learning/stack/bc-10k_steps-06-04-24_10_06_35-333b32a8-c019-4fed-9b8f-1ce59166bb2b:latest",
            "include_absorbing_state": true
        },
        "residual": {
            "architecture": "mlp",
            "layers": [256, 256, 256],
            "activation": "tanh",
            "flatten": true,
            "std_transform": "squareplus",
            "policy_distribution": "squashed_gaussian"
        },
        "qf": {
            "architecture": "ensemble",
            "model": {
                "architecture": "mlp",
                "layers": [2048, 2048],
                "flatten": true,
                "use_batch_norm": true
            },
            "num_models": 2,
            "vmap_all": false
        },
        "qf_encoding": {
            "q_function": "state_action_input",
            "type": "concatenate_inputs_encoding",
            "kwargs": {}
        }
    },
    "optimizer_config": {
        "residual": {
            "optimizer": "adam",
            "lr": {
                "scheduler": "constant_schedule",
                "scheduler_kwargs": {
                    "value": 0.0003
                }
            },
            "max_grad_norm": false
        },
        "qf": {
            "optimizer": "adam",
            "lr": {
                "scheduler": "constant_schedule",
                "scheduler_kwargs": {
                    "value": 0.0003
                }
            },
            "max_grad_norm": false
        },
        "temp": {
            "optimizer": "adam",
            "lr": {
                "scheduler": "constant_schedule",
                "scheduler_kwargs": {
                    "value": 0.0003
                }
            },
            "max_grad_norm": false
        }
    },
    "learner_config": {
        "task": "residual",
        "env_config": {
            "env_type": "manipulator_learning",
            "env_name": "PandaPlayInsertTrayXYZState",
            "env_kwargs": {
                "main_task": "stack",
                "dense_reward": false
            }
        },
        "seeds": {
            "model_seed": 42,
            "buffer_seed": 42,
            "demo_buffer_seed": 43,
            "env_seed": 42,
            "learner_seed": 42
        },
        "buffer_config": {
            "buffer_type": "default",
            "buffer_size": 1000000
        },
        "demo_buffer_config": {
            "load_buffer": "/home/bryan/research/lfgp/lfgp_data/custom_expert_data/stack/1000000_steps_no_extra_final/int_0.gz",
            "buffer_type": "default",
            "set_size": 10000
        },
        "buffer_warmup": 1000,
        "pretrain_steps": 5000,
        "random_explore_action": false,
        "num_steps_per_epoch": 5000,
        "batch_size": 256,
        "learner": "sac",
        "variant": "rlpd:cross_q",
        "remove_demo_absorbing_state": true,
        "obs_rms": false,
        "value_rms": false,
        "gamma": 0.99,
        "update_frequency": 1,
        "target_entropy": "auto",
        "initial_temperature": 0.2,
        "pretrain_qf_loss_setting": {
            "coefficient": 1.0,
            "reduction": "mean",
            "include_entropy_regularization": false,
            "cal_q_regularizer": 1.0,
            "cal_q_max_return": 360
        },
        "qf_loss_setting": {
            "coefficient": 1.0,
            "reduction": "mean",
            "include_entropy_regularization": false
        },
        "pi_loss_setting": {
            "coefficient": 1.0,
            "reduction": "mean"
        },
        "temp_loss_setting": {
            "coefficient": 1.0,
            "reduction": "mean"
        },
        "num_qf_updates": 1,
        "actor_update_frequency": 20,
        "target_update_frequency": 1,
        "tau": 5e-3
    },
    "train_config": {
        "num_epochs": 1000
    }
}