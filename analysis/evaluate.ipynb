{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.random as jrandom\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from jaxl.constants import *\n",
    "from jaxl.models.utils import (\n",
    "    get_model,\n",
    "    load_config,\n",
    "    load_params,\n",
    "    get_wsrl_model,\n",
    "    iterate_params,\n",
    "    get_policy,\n",
    "    policy_output_dim,\n",
    "    get_residual_policy,\n",
    ")\n",
    "from jaxl.envs import get_environment\n",
    "from jaxl.envs.rollouts import EvaluationRollout\n",
    "from jaxl.utils import get_device, parse_dict\n",
    "\n",
    "# get_device(\"gpu:0\")\n",
    "get_device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = \"/home/bryan/research/jaxl/logs/manipulator_learning\"\n",
    "\n",
    "ablation_name = \"stack\"\n",
    "# learner_name = \"cross_q-sac-06-03-24_16_53_41-9dd96b95-aefd-44fd-8894-4854c9c08abf\"\n",
    "# learner_name = \"bc-06-04-24_09_43_02-a09b01c4-e33d-4e88-9eda-d7d36a68cdb8\"\n",
    "# learner_name = \"bc-100k_steps-06-04-24_09_57_35-e8bd5a54-9148-41a9-ace8-f33c5cfbab9f\"\n",
    "learner_name = \"bc-10k_steps-06-04-24_10_06_35-333b32a8-c019-4fed-9b8f-1ce59166bb2b\"\n",
    "# learner_name = \"warm_start_reinforce-06-04-24_13_28_32-b356a022-d53a-4b11-9726-ccfe4dca0777\"\n",
    "# learner_name = \"rlpd-sac-06-05-24_16_15_56-c5ad96da-4ac4-466b-a221-74cfea71bd19\"\n",
    "# learner_name = (\n",
    "#     \"residual-rlpd-sac-06-06-24_17_55_49-1e0d722f-7e5d-4310-95de-2480ad35ab72\"\n",
    "# )\n",
    "# learner_name = \"residual-rlpd-sac-fixed_temp-06-07-24_09_25_20-c64a33d6-de4f-4cae-96e9-e4a09ef9b50c\"\n",
    "learner_name = \"residual-rlpd-cross_q-deterministic_exploration-wide_critic-06-10-24_11_12_20-c40f7bbb-80de-47a6-b02f-a3568dd1a877\"\n",
    "\n",
    "learner_path = os.path.join(result_dir, ablation_name, learner_name)\n",
    "\n",
    "checkpoint = \"latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, config = load_config(learner_path)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = {\n",
    "    \"env_type\": \"manipulator_learning\",\n",
    "    \"env_name\": \"PandaPlayInsertTrayXYZState\",\n",
    "    \"env_kwargs\": {\"main_task\": \"stack\", \"dense_reward\": False},\n",
    "}\n",
    "env = get_environment(parse_dict(env_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_absorbing_state = False\n",
    "if config.learner_config.task == CONST_RESIDUAL:\n",
    "    backbone_act_dim = policy_output_dim(env.act_dim, config.model_config.backbone)\n",
    "    residual_act_dim = policy_output_dim(env.act_dim, config.model_config.residual)\n",
    "\n",
    "    backbone_model = get_model(\n",
    "        env.observation_space.shape, env.act_dim, config.model_config.backbone\n",
    "    )\n",
    "    residual_model = get_model(\n",
    "        env.observation_space.shape, residual_act_dim, config.model_config.residual\n",
    "    )\n",
    "    policy = get_residual_policy(\n",
    "        backbone_model,\n",
    "        residual_model,\n",
    "        config.model_config,\n",
    "    )\n",
    "else:\n",
    "    model_out_dim = policy_output_dim(env.act_dim, config.learner_config)\n",
    "    if config.learner_config.learner == CONST_BC:\n",
    "        model = get_model(\n",
    "            int(np.prod(env.observation_space.shape)) + 1,\n",
    "            env.act_dim,\n",
    "            config.model_config,\n",
    "        )\n",
    "        include_absorbing_state = True\n",
    "    elif config.learner_config.task == CONST_WSRL:\n",
    "        model = get_wsrl_model(\n",
    "            env.observation_space.shape, model_out_dim, config.model_config.policy\n",
    "        )\n",
    "        include_absorbing_state = True\n",
    "    else:\n",
    "        model = get_model(\n",
    "            env.observation_space.shape, model_out_dim, config.model_config.policy\n",
    "        )\n",
    "    policy = get_policy(model, config.learner_config)\n",
    "\n",
    "params = load_params(f\"{learner_path}:{checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_episodes = 100\n",
    "eval_seed = 42\n",
    "render = False\n",
    "random = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = EvaluationRollout(env, eval_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout.rollout(\n",
    "    params[CONST_MODEL_DICT][CONST_MODEL][CONST_POLICY],\n",
    "    policy,\n",
    "    False,\n",
    "    total_episodes,\n",
    "    random=random,\n",
    "    render=render,\n",
    "    include_absorbing_state=include_absorbing_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rollout.episodic_returns), np.std(rollout.episodic_returns), np.sum(\n",
    "    np.array(rollout.episodic_returns) > 0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10k: (203.98, 147.88853775732588)  \n",
    "100k: (253.7, 127.34115595517422)  \n",
    "1M: (215.96, 148.52123888521803)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Residual RLPD @ 300 deterministic: (208.96, 150.14752212407635)\n",
    "Residual RLPD @ 400 deterministic: (210.16, 137.68796025796883)\n",
    "Residual RLPD @ 400 stochastic: (213.56, 135.53481619126504)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Residual RLPD with fixed temp @ initialization random: (130.32, 139.2440217747247)\n",
    "Residual RLPD with fixed temp @ 10 random: (125.94, 150.11827470364827)\n",
    "Residual RLPD with fixed temp @ 20 random: (224.86, 137.30244134755944)\n",
    "Residual RLPD with fixed temp @ 150 random: (252.2, 125.54106897744657)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Residual RLPD CrossQ @ 160 deterministic: (280.46, 94.8232482042247, 46)\n",
    "Residual RLPD CrossQ @ latest deterministic: (281.26, 97.1332713337711, 45)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "BC @ 10k: (188.91, 146.51594418355978, 64)\n",
    "Residual RLPD CrossQ @ latest deterministic: (272.54, 107.02592396237466, 87)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
